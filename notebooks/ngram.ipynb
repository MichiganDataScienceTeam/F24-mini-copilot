{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Autocompletion with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We frame the code autocompletion task as follows: given a sequence of $n$ tokens (which you can consider $n$ words of code), predict the $n+1$th token. With this interpretation of the problem in mind, we can use a simple n-gram based maximum likelihood predictor as a rough heuristic for code autocompletion. There are many issues with this approach, but it helps build the intuition of *next token prediction.* Next token prediction is the underlying concept behind Large Language Models, and is one approach that we will be using to solve the code autocompletion problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can start by pulling our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n: int) -> Dataset:\n",
    "    \"\"\"Pull n samples of the codeparrot dataset onto memory, after filtering for Python code.\"\"\"\n",
    "    # https://huggingface.co/datasets/codeparrot/github-code\n",
    "    # Load the dataset\n",
    "    ds = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")\n",
    "\n",
    "    # TODO: Filter the dataset for only Python code\n",
    "    ...\n",
    "\n",
    "    ds = ds.take(n)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# We will only load 1,000 samples for now\n",
    "dataset = get_data(1_000)\n",
    "\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to tokenize our dataset, as the individual \"grams\" in our n-gram model is built upon tokens. If you are unfamiliar with tokenization, [this](https://medium.com/@abdallahashraf90x/tokenization-in-nlp-all-you-need-to-know-45c00cfa2df7) is a quick read and [this is](https://youtu.be/zduSFxRajkE?si=4TAlVacyZTNUmLn9) a long but incredibly comprehensive watch that you will not regret (Andrej Karpathy is the goat). If you are familiar with tokenization, you likely do not appreciate it enough.\n",
    "\n",
    "There is a lot to consider when tokenizing a code dataset. For instance, we cannot just naively split based on whitespace - newlines and punctuation are quite important in code, and we may want them to be treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "# TODO: Tokenize the code snippets\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the tokens produced\n",
    "for token in all_tokens[410:450]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build out our n-grams. In this notebook, we will use 3-grams, and only 3-grams. A better approach would be to leverage a mixture of uni, bi, tri, ..., n grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(n: int, tokens: list) -> defaultdict:\n",
    "    \"\"\"Given a list of tokens, return a dictionary of all the n-grams from the tokens.\"\"\"\n",
    "    ngrams = []\n",
    "\n",
    "    # TODO: Implement the n-gram generation here\n",
    "    ...\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "three_grams = get_ngrams(3, all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on all of the 3-grams we have derived, we can build a simple count based model we can use to predict the next token given the past 2 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3gram = ... # TODO: Initialize the 3gram model\n",
    "\n",
    "for three_gram in three_grams:\n",
    "    # TODO: Update the model with the 3-grams\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3gram[(\"def\", \"main\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model: dict, w1: str, w2: str, top_n=10):\n",
    "    \"\"\"Plot the top_n words that follow the bigram (w1, w2) in the model.\"\"\"\n",
    "    words = [w for w, _ in sorted(model[(w1, w2)].items(), key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "    counts = [c for _, c in sorted(model[(w1, w2)].items(), key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(words)))\n",
    "    plt.bar(words, counts, color=colors)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3gram, \"def\", \"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3gram, \"import\", \"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3gram, \"from\", \"django\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a simple function that uses the most probable next token to generate a completion based on a provided input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: dict, w1: str, w2: str, n: int = 10) -> list:\n",
    "    \"\"\"Generate the next n token from the model, given the initial bigram (w1, w2).\"\"\"\n",
    "    # TODO: Implement the generation here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"def main\"\n",
    "input_text = input_text.split()\n",
    "generate(model_3gram, *input_text, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"import pandas\"\n",
    "input_text = input_text.split()\n",
    "generate(model_3gram, *input_text, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should sensible output using the 3-gram model.\n",
    "\n",
    "Now, on your own, try to use a mixture of different n-gram models to produce better output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build other n-gram models, generate text from them,\n",
    "# and eventually combine them to generate better snippets of code.\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
