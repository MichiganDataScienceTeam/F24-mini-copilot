{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Autocompletion with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is an autoregressive model trained on a Causal Language Modeling task. This menas that the GPT-2 model was trained on a next token prediction task, such that the model, provided a sequence of $n$ tokens, had to predict the $n+1$*th* token. This is a Causal Language Modeling task since the prediction of the $n+1$*th* token can be framed as the below probabilistic task:\n",
    "\n",
    "$$t_{n+1} = \\argmax_{x} \\Pr(x∣t_1,t_2,…,t_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By giving this model a sequence of code ($n$ tokens of code, to be specific), we can expect to receive what, probabilistically, the next bit of code should be (the $n+1$*th* token). Once the model predicts the $n+1$*th* token, we can use this new sequence of tokens $[t_0, ..., t_{n+1}]$ to predict the $n+2$*th* token, and this process can be repeated recursively to generate as many tokens as we would like. This is known as autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "# these are all the libraries you'd need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(inp: str) -> str:\n",
    "    \"\"\"OPTIONAL: Perform data cleaning, if necessary.\"\"\"\n",
    "    ...\n",
    "\n",
    "def get_data() -> Dataset:\n",
    "    # https://huggingface.co/datasets/codeparrot/codeparrot-clean\n",
    "    # Load the dataset\n",
    "    ds = load_dataset(\"codeparrot/codeparrot-clean\", streaming=True, trust_remote_code=True, split=\"train\")\n",
    "\n",
    "    # Clean the data\n",
    "    # ds = ds.map(lambda x: {\"content\": clean_data(x[\"content\"])})\n",
    "\n",
    "    return ds\n",
    "\n",
    "dataset = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset) # This is important..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_data(dataset: Dataset) -> (Dataset, Dataset):\n",
    "    \"\"\"TODO: Split the dataset into training and validation sets.\"\"\"\n",
    "    # This is not too straightforward because the dataset is a streaming dataset\n",
    "    raise NotImplementedError\n",
    "\n",
    "train_data, valid_data = get_train_valid_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Wrapper to account for download errors so training doesn't stop due to error pulling data from HF.\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        while True:\n",
    "            try:\n",
    "                item = next(iterator)\n",
    "                yield item\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception during data loading: {e}. Skipping item.\")\n",
    "                continue\n",
    "\n",
    "train_data = SafeIterableDataset(train_data)\n",
    "valid_data = SafeIterableDataset(valid_data)\n",
    "\n",
    "train_loader = DataLoader(train_data,  batch_size=16)\n",
    "test_loader  = DataLoader(valid_data,  batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp: list[str]):\n",
    "    \"\"\"\n",
    "    TODO: Tokenize the input.\n",
    "    Consider:\n",
    "    - Padding?\n",
    "    - Truncation?\n",
    "    - Anything else?\n",
    "    \"\"\"\n",
    "    ...\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # TODO: Implement training loop\n",
    "        # Note that device that data is on should be the same as the model\n",
    "        ...\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # TODO: Implement validation loop\n",
    "            # Note that device that data is on should be the same as the model\n",
    "            ...\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_ETAG_TIMEOUT\"]     = \"500\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider setting up model checkpointing (set up a directory to save checkpoints)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear residual gradients (might cause issues with taking grad. of frozen layers)\n",
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    # TODO: Implement training and validation\n",
    "    ...\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common antidotes to CUDA Out of Memory errors include:\n",
    "1. Freezing layers of your model (training less parameters).\n",
    "2. Using gradient checkpointing to save GPU memory.\n",
    "3. Reducing the max sequence length of your data (default=1024 with GPT-2 tokenizer, which is colossal).\n",
    "4. Reducing batch size (look into gradient accumulation).\n",
    "\n",
    "And, of course:\n",
    "\n",
    "5. Using a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the model\n",
    "...\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
