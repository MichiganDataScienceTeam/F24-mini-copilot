{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Autocompletion with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is an autoregressive model trained on a Causal Language Modeling task. This menas that the GPT-2 model was trained on a next token prediction task, such that the model, provided a sequence of $n$ tokens, had to predict the $n+1$*th* token. This is a Causal Language Modeling task since the prediction of the $n+1$*th* token can be framed as the below probabilistic task:\n",
    "\n",
    "$$t_{n+1} = \\argmax_{x} \\Pr(x∣t_1,t_2,…,t_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By giving this model a sequence of code ($n$ tokens of code, to be specific), we can expect to receive what, probabilistically, the next bit of code should be (the $n+1$*th* token). Once the model predicts the $n+1$*th* token, we can use this new sequence of tokens $[t_0, ..., t_{n+1}]$ to predict the $n+2$*th* token, and this process can be repeated recursively to generate as many tokens as we would like. This is known as autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennisfj/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "# these are all the libraries you'd need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_data(inp: str) -> str:\n",
    "    \"\"\"OPTIONAL: Perform data cleaning, if necessary.\"\"\"\n",
    "    s = re.sub(r'^#.*\\n?', '', inp, flags=re.MULTILINE)\n",
    "    return s\n",
    "\n",
    "def get_data() -> Dataset:\n",
    "    # https://huggingface.co/datasets/codeparrot/codeparrot-clean\n",
    "    # Load the dataset\n",
    "    ds = load_dataset(\"codeparrot/codeparrot-clean\", streaming=True, trust_remote_code=True, split=\"train\")\n",
    "\n",
    "    # Clean the data\n",
    "    ds = ds.map(lambda x: {\"content\": clean_data(x[\"content\"])})\n",
    "\n",
    "    return ds\n",
    "\n",
    "dataset = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.iterable_dataset.IterableDataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset) # This is important..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean out comments\n",
    "Retrain tokenizer (specify vocabulary size)\n",
    "tokenize data (bucket code, chunk)\n",
    "perplexity (lower is better, 0-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model     = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay= 0.001)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_data(dataset: Dataset) -> (Dataset, Dataset):\n",
    "    \"\"\"TODO: Split the dataset into training and validation sets.\"\"\"\n",
    "    # This is not too straightforward because the dataset is a streaming dataset\n",
    "    #n = 300000\n",
    "    n = 150\n",
    "    split = int(n*0.75)\n",
    "    dataset.shuffle()\n",
    "    ds_train = dataset.take(n)\n",
    "    ds_valid = ds_train.skip(split)\n",
    "    ds_train = ds_train.take(split)\n",
    "    return ds_train, ds_valid\n",
    "\n",
    "train_data, valid_data = get_train_valid_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.iterable_dataset.IterableDataset'>\n",
      "<class 'datasets.iterable_dataset.IterableDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeIterableDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Wrapper to account for download errors so training doesn't stop due to error pulling data from HF.\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        while True:\n",
    "            try:\n",
    "                item = next(iterator)\n",
    "                yield item\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception during data loading: {e}. Skipping item.\")\n",
    "                continue\n",
    "\n",
    "train_data = SafeIterableDataset(train_data)\n",
    "valid_data = SafeIterableDataset(valid_data)\n",
    "\n",
    "train_loader = DataLoader(train_data,  batch_size=16)\n",
    "test_loader  = DataLoader(valid_data,  batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp: list[str]):\n",
    "    \"\"\"\n",
    "    TODO: Tokenize the input.\n",
    "    Consider:\n",
    "    - Padding?\n",
    "    - Truncation?\n",
    "    - Anything else?\n",
    "    \"\"\"\n",
    "    # truncate to first 256 tokens\n",
    "    # pad to make every example the same size (ex: 256 tokens)\n",
    "    inp = tokenizer(inp)['input_ids']\n",
    "    results = []\n",
    "\n",
    "    for ex in inp:\n",
    "        ex.extend([0] * (max(0, 256 - len(ex))))\n",
    "        ex = ex[:256]\n",
    "        results.append(torch.tensor(ex))\n",
    "    return torch.stack(results)\n",
    "\n",
    "\n",
    "    #return(tokenizer(inp)[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "x = tokenize([\"import pandas as pd\", \"import numpy as np\", \"def sum(a, b):\\n\\treturn a + b\"])\n",
    "#print(x.shape)\n",
    "for i in x:\n",
    "    print(i.shape)\n",
    "#for i in x:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        #single_example_split = split_into_groups_of_256_tokens(tokenized_example)\n",
    "        #for _265_token_example in example_split:\n",
    "        # TODO: Implement training loop\n",
    "        # Note that device that data is on should be the same as the model\n",
    "        #for k in batch:\n",
    "            #print(k, batch[k])\n",
    "        input_ids = tokenize(batch[\"content\"])\n",
    "        labels = input_ids.clone()\n",
    "        # labels are automatically shifted for next token prediction\n",
    "        # assuming model is of type AutoModelForCausalLM\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #raise NotImplementedError\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.62711763381958\n",
      "perplexity: 755.302001953125\n"
     ]
    }
   ],
   "source": [
    "def val():\n",
    "    losses=[]\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Implement validation loop\n",
    "            # Note that device that data is on should be the same as the model\n",
    "            input_ids = tokenize(batch[\"content\"])\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            losses.append(outputs.loss)\n",
    "        loss = torch.mean(torch.tensor(losses))\n",
    "        try:\n",
    "            perplexity = torch.exp(loss)\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "            raise NotImplementedError\n",
    "        return loss.item(), perplexity.item()\n",
    "\n",
    "loss, perplexity = val()\n",
    "print(\"loss:\", loss)\n",
    "print(\"perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'repo_name': ['derekjchow/models', 'lancezlin/ml_template_py', '14thibea/megamix', 'sgarrity/bedrock', 'kevinmel2000/brython', 'rhololkeolke/apo-website-devin', 'korotkyn/ibis', 'hectord/lettuce', 'ARM-software/mbed-beetle', 'ahmadio/edx-platform', 'goliate/sarakha63-persomov', 'longman694/youtube-dl', 'mesocentrefc/easybuild-framework', 'kinghaitao/git-core', 'broferek/ansible', 'wavelets/zipline'], 'path': ['research/deeplab/core/nas_cell.py', 'lib/python2.7/site-packages/sklearn/metrics/tests/test_score_objects.py', 'megamix/batch/DPGMM.py', 'lib/l10n_utils/management/commands/fluent.py', 'www/src/Lib/test/unittests/test_cgitb.py', 'src/application/facebook/facebook.py', 'ibis/expr/tests/test_temporal.py', 'tests/integration/lib/Django-1.2.5/django/core/handlers/base.py', 'tools/host_tests/host_tests_plugins/module_copy_smart.py', 'lms/lib/courseware_search/lms_filter_generator.py', 'couchpotato/core/media/movie/providers/trailer/youtube_dl/extractor/twentyfourvideo.py', 'youtube_dl/extractor/watchbox.py', 'easybuild/tools/version.py', 'scripts/rt-tester/rt-tester.py', 'test/units/modules/network/f5/test_bigip_service_policy.py', 'zipline/examples/dual_ema_talib.py'], 'copies': ['1', '15', '1', '8', '113', '2', '9', '44', '2', '58', '32', '14', '2', '904', '38', '2'], 'size': ['8432', '17443', '20644', '3597', '2551', '8731', '5955', '9926', '4378', '5634', '3892', '5539', '2926', '5366', '4128', '3230'], 'content': ['\\n\"\"\"Cell structure used by NAS.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport tensorflow as tf\\nfrom deeplab.core.utils import resize_bilinear\\nfrom deeplab.core.utils import scale_dimension\\n\\narg_scope = tf.contrib.framework.arg_scope\\nslim = tf.contrib.slim\\n\\n\\nclass NASBaseCell(object):\\n  \"\"\"NASNet Cell class that is used as a \\'layer\\' in image architectures.\\n  See https://arxiv.org/abs/1707.07012 and https://arxiv.org/abs/1712.00559.\\n\\n  Args:\\n    num_conv_filters: The number of filters for each convolution operation.\\n    operations: List of operations that are performed in the NASNet Cell in\\n      order.\\n    used_hiddenstates: Binary array that signals if the hiddenstate was used\\n      within the cell. This is used to determine what outputs of the cell\\n      should be concatenated together.\\n    hiddenstate_indices: Determines what hiddenstates should be combined\\n      together with the specified operations to create the NASNet cell.\\n  \"\"\"\\n\\n  def __init__(self, num_conv_filters, operations, used_hiddenstates,\\n               hiddenstate_indices, drop_path_keep_prob, total_num_cells,\\n               total_training_steps):\\n    if len(hiddenstate_indices) != len(operations):\\n      raise ValueError(\\n          \\'Number of hiddenstate_indices and operations should be the same.\\')\\n    if len(operations) % 2:\\n      raise ValueError(\\'Number of operations should be even.\\')\\n    self._num_conv_filters = num_conv_filters\\n    self._operations = operations\\n    self._used_hiddenstates = used_hiddenstates\\n    self._hiddenstate_indices = hiddenstate_indices\\n    self._drop_path_keep_prob = drop_path_keep_prob\\n    self._total_num_cells = total_num_cells\\n    self._total_training_steps = total_training_steps\\n\\n  def __call__(self, net, scope, filter_scaling, stride, prev_layer, cell_num):\\n    \"\"\"Runs the conv cell.\"\"\"\\n    self._cell_num = cell_num\\n    self._filter_scaling = filter_scaling\\n    self._filter_size = int(self._num_conv_filters * filter_scaling)\\n\\n    with tf.variable_scope(scope):\\n      net = self._cell_base(net, prev_layer)\\n      for i in range(len(self._operations) // 2):\\n        with tf.variable_scope(\\'comb_iter_{}\\'.format(i)):\\n          h1 = net[self._hiddenstate_indices[i * 2]]\\n          h2 = net[self._hiddenstate_indices[i * 2 + 1]]\\n          with tf.variable_scope(\\'left\\'):\\n            h1 = self._apply_conv_operation(\\n                h1, self._operations[i * 2], stride,\\n                self._hiddenstate_indices[i * 2] < 2)\\n          with tf.variable_scope(\\'right\\'):\\n            h2 = self._apply_conv_operation(\\n                h2, self._operations[i * 2 + 1], stride,\\n                self._hiddenstate_indices[i * 2 + 1] < 2)\\n          with tf.variable_scope(\\'combine\\'):\\n            h = h1 + h2\\n          net.append(h)\\n\\n      with tf.variable_scope(\\'cell_output\\'):\\n        net = self._combine_unused_states(net)\\n\\n      return net\\n\\n  def _cell_base(self, net, prev_layer):\\n    \"\"\"Runs the beginning of the conv cell before the chosen ops are run.\"\"\"\\n    filter_size = self._filter_size\\n\\n    if prev_layer is None:\\n      prev_layer = net\\n    else:\\n      if net.shape[2] != prev_layer.shape[2]:\\n        prev_layer = resize_bilinear(\\n            prev_layer, tf.shape(net)[1:3], prev_layer.dtype)\\n      if filter_size != prev_layer.shape[3]:\\n        prev_layer = tf.nn.relu(prev_layer)\\n        prev_layer = slim.conv2d(prev_layer, filter_size, 1, scope=\\'prev_1x1\\')\\n        prev_layer = slim.batch_norm(prev_layer, scope=\\'prev_bn\\')\\n\\n    net = tf.nn.relu(net)\\n    net = slim.conv2d(net, filter_size, 1, scope=\\'1x1\\')\\n    net = slim.batch_norm(net, scope=\\'beginning_bn\\')\\n    net = tf.split(axis=3, num_or_size_splits=1, value=net)\\n    net.append(prev_layer)\\n    return net\\n\\n  def _apply_conv_operation(self, net, operation, stride,\\n                            is_from_original_input):\\n    \"\"\"Applies the predicted conv operation to net.\"\"\"\\n    if stride > 1 and not is_from_original_input:\\n      stride = 1\\n    input_filters = net.shape[3]\\n    filter_size = self._filter_size\\n    if \\'separable\\' in operation:\\n      num_layers = int(operation.split(\\'_\\')[-1])\\n      kernel_size = int(operation.split(\\'x\\')[0][-1])\\n      for layer_num in range(num_layers):\\n        net = tf.nn.relu(net)\\n        net = slim.separable_conv2d(\\n            net,\\n            filter_size,\\n            kernel_size,\\n            depth_multiplier=1,\\n            scope=\\'separable_{0}x{0}_{1}\\'.format(kernel_size, layer_num + 1),\\n            stride=stride)\\n        net = slim.batch_norm(\\n            net, scope=\\'bn_sep_{0}x{0}_{1}\\'.format(kernel_size, layer_num + 1))\\n        stride = 1\\n    elif \\'atrous\\' in operation:\\n      kernel_size = int(operation.split(\\'x\\')[0][-1])\\n      net = tf.nn.relu(net)\\n      if stride == 2:\\n        scaled_height = scale_dimension(tf.shape(net)[1], 0.5)\\n        scaled_width = scale_dimension(tf.shape(net)[2], 0.5)\\n        net = resize_bilinear(net, [scaled_height, scaled_width], net.dtype)\\n        net = slim.conv2d(net, filter_size, kernel_size, rate=1,\\n                          scope=\\'atrous_{0}x{0}\\'.format(kernel_size))\\n      else:\\n        net = slim.conv2d(net, filter_size, kernel_size, rate=2,\\n                          scope=\\'atrous_{0}x{0}\\'.format(kernel_size))\\n      net = slim.batch_norm(net, scope=\\'bn_atr_{0}x{0}\\'.format(kernel_size))\\n    elif operation in [\\'none\\']:\\n      if stride > 1 or (input_filters != filter_size):\\n        net = tf.nn.relu(net)\\n        net = slim.conv2d(net, filter_size, 1, stride=stride, scope=\\'1x1\\')\\n        net = slim.batch_norm(net, scope=\\'bn_1\\')\\n    elif \\'pool\\' in operation:\\n      pooling_type = operation.split(\\'_\\')[0]\\n      pooling_shape = int(operation.split(\\'_\\')[-1].split(\\'x\\')[0])\\n      if pooling_type == \\'avg\\':\\n        net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=\\'SAME\\')\\n      elif pooling_type == \\'max\\':\\n        net = slim.max_pool2d(net, pooling_shape, stride=stride, padding=\\'SAME\\')\\n      else:\\n        raise ValueError(\\'Unimplemented pooling type: \\', pooling_type)\\n      if input_filters != filter_size:\\n        net = slim.conv2d(net, filter_size, 1, stride=1, scope=\\'1x1\\')\\n        net = slim.batch_norm(net, scope=\\'bn_1\\')\\n    else:\\n      raise ValueError(\\'Unimplemented operation\\', operation)\\n\\n    if operation != \\'none\\':\\n      net = self._apply_drop_path(net)\\n    return net\\n\\n  def _combine_unused_states(self, net):\\n    \"\"\"Concatenates the unused hidden states of the cell.\"\"\"\\n    used_hiddenstates = self._used_hiddenstates\\n    states_to_combine = ([\\n        h for h, is_used in zip(net, used_hiddenstates) if not is_used])\\n    net = tf.concat(values=states_to_combine, axis=3)\\n    return net\\n\\n  @tf.contrib.framework.add_arg_scope\\n  def _apply_drop_path(self, net):\\n    \"\"\"Apply drop_path regularization.\"\"\"\\n    drop_path_keep_prob = self._drop_path_keep_prob\\n    if drop_path_keep_prob < 1.0:\\n      # Scale keep prob by layer number.\\n      assert self._cell_num != -1\\n      layer_ratio = (self._cell_num + 1) / float(self._total_num_cells)\\n      drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\\n      # Decrease keep prob over time.\\n      current_step = tf.cast(tf.train.get_or_create_global_step(), tf.float32)\\n      current_ratio = tf.minimum(1.0, current_step / self._total_training_steps)\\n      drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob))\\n      # Drop path.\\n      noise_shape = [tf.shape(net)[0], 1, 1, 1]\\n      random_tensor = drop_path_keep_prob\\n      random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)\\n      binary_tensor = tf.cast(tf.floor(random_tensor), net.dtype)\\n      keep_prob_inv = tf.cast(1.0 / drop_path_keep_prob, net.dtype)\\n      net = net * keep_prob_inv * binary_tensor\\n    return net\\n', 'import pickle\\nimport tempfile\\nimport shutil\\nimport os\\nimport numbers\\n\\nimport numpy as np\\n\\nfrom sklearn.utils.testing import assert_almost_equal\\nfrom sklearn.utils.testing import assert_array_equal\\nfrom sklearn.utils.testing import assert_raises\\nfrom sklearn.utils.testing import assert_raises_regexp\\nfrom sklearn.utils.testing import assert_true\\nfrom sklearn.utils.testing import ignore_warnings\\nfrom sklearn.utils.testing import assert_not_equal\\nfrom sklearn.utils.testing import assert_warns_message\\n\\nfrom sklearn.base import BaseEstimator\\nfrom sklearn.metrics import (f1_score, r2_score, roc_auc_score, fbeta_score,\\n                             log_loss, precision_score, recall_score)\\nfrom sklearn.metrics.cluster import adjusted_rand_score\\nfrom sklearn.metrics.scorer import (check_scoring, _PredictScorer,\\n                                    _passthrough_scorer)\\nfrom sklearn.metrics import make_scorer, get_scorer, SCORERS\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.dummy import DummyRegressor\\nfrom sklearn.linear_model import Ridge, LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.datasets import make_multilabel_classification\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.externals import joblib\\n\\n\\nREGRESSION_SCORERS = [\\'r2\\', \\'neg_mean_absolute_error\\',\\n                      \\'neg_mean_squared_error\\', \\'neg_median_absolute_error\\',\\n                      \\'mean_absolute_error\\',\\n                      \\'mean_squared_error\\', \\'median_absolute_error\\']\\n\\nCLF_SCORERS = [\\'accuracy\\', \\'f1\\', \\'f1_weighted\\', \\'f1_macro\\', \\'f1_micro\\',\\n               \\'roc_auc\\', \\'average_precision\\', \\'precision\\',\\n               \\'precision_weighted\\', \\'precision_macro\\', \\'precision_micro\\',\\n               \\'recall\\', \\'recall_weighted\\', \\'recall_macro\\', \\'recall_micro\\',\\n               \\'neg_log_loss\\', \\'log_loss\\',\\n               \\'adjusted_rand_score\\'  # not really, but works\\n               ]\\n\\nMULTILABEL_ONLY_SCORERS = [\\'precision_samples\\', \\'recall_samples\\', \\'f1_samples\\']\\n\\n\\ndef _make_estimators(X_train, y_train, y_ml_train):\\n    # Make estimators that make sense to test various scoring methods\\n    sensible_regr = DummyRegressor(strategy=\\'median\\')\\n    sensible_regr.fit(X_train, y_train)\\n    sensible_clf = DecisionTreeClassifier(random_state=0)\\n    sensible_clf.fit(X_train, y_train)\\n    sensible_ml_clf = DecisionTreeClassifier(random_state=0)\\n    sensible_ml_clf.fit(X_train, y_ml_train)\\n    return dict(\\n        [(name, sensible_regr) for name in REGRESSION_SCORERS] +\\n        [(name, sensible_clf) for name in CLF_SCORERS] +\\n        [(name, sensible_ml_clf) for name in MULTILABEL_ONLY_SCORERS]\\n    )\\n\\n\\nX_mm, y_mm, y_ml_mm = None, None, None\\nESTIMATORS = None\\nTEMP_FOLDER = None\\n\\n\\ndef setup_module():\\n    # Create some memory mapped data\\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\\n    TEMP_FOLDER = tempfile.mkdtemp(prefix=\\'sklearn_test_score_objects_\\')\\n    X, y = make_classification(n_samples=30, n_features=5, random_state=0)\\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\\n                                             random_state=0)\\n    filename = os.path.join(TEMP_FOLDER, \\'test_data.pkl\\')\\n    joblib.dump((X, y, y_ml), filename)\\n    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode=\\'r\\')\\n    ESTIMATORS = _make_estimators(X_mm, y_mm, y_ml_mm)\\n\\n\\ndef teardown_module():\\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\\n    # GC closes the mmap file descriptors\\n    X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None\\n    shutil.rmtree(TEMP_FOLDER)\\n\\n\\nclass EstimatorWithoutFit(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    pass\\n\\n\\nclass EstimatorWithFit(BaseEstimator):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        return self\\n\\n\\nclass EstimatorWithFitAndScore(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        return self\\n\\n    def score(self, X, y):\\n        return 1.0\\n\\n\\nclass EstimatorWithFitAndPredict(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        self.y = y\\n        return self\\n\\n    def predict(self, X):\\n        return self.y\\n\\n\\nclass DummyScorer(object):\\n    \"\"\"Dummy scorer that always returns 1.\"\"\"\\n    def __call__(self, est, X, y):\\n        return 1\\n\\n\\ndef test_all_scorers_repr():\\n    # Test that all scorers have a working repr\\n    for name, scorer in SCORERS.items():\\n        repr(scorer)\\n\\n\\ndef test_check_scoring():\\n    # Test all branches of check_scoring\\n    estimator = EstimatorWithoutFit()\\n    pattern = (r\"estimator should be an estimator implementing \\'fit\\' method,\"\\n               r\" .* was passed\")\\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\\n\\n    estimator = EstimatorWithFitAndScore()\\n    estimator.fit([[1]], [1])\\n    scorer = check_scoring(estimator)\\n    assert_true(scorer is _passthrough_scorer)\\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\\n\\n    estimator = EstimatorWithFitAndPredict()\\n    estimator.fit([[1]], [1])\\n    pattern = (r\"If no scoring is specified, the estimator passed should have\"\\n               r\" a \\'score\\' method\\\\. The estimator .* does not\\\\.\")\\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\\n\\n    scorer = check_scoring(estimator, \"accuracy\")\\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\\n\\n    estimator = EstimatorWithFit()\\n    scorer = check_scoring(estimator, \"accuracy\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    estimator = EstimatorWithFit()\\n    scorer = check_scoring(estimator, allow_none=True)\\n    assert_true(scorer is None)\\n\\n\\ndef test_check_scoring_gridsearchcv():\\n    # test that check_scoring works on GridSearchCV and pipeline.\\n    # slightly redundant non-regression test.\\n\\n    grid = GridSearchCV(LinearSVC(), param_grid={\\'C\\': [.1, 1]})\\n    scorer = check_scoring(grid, \"f1\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    pipe = make_pipeline(LinearSVC())\\n    scorer = check_scoring(pipe, \"f1\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    # check that cross_val_score definitely calls the scorer\\n    # and doesn\\'t make any assumptions about the estimator apart from having a\\n    # fit.\\n    scores = cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1],\\n                             scoring=DummyScorer())\\n    assert_array_equal(scores, 1)\\n\\n\\ndef test_make_scorer():\\n    # Sanity check on the make_scorer factory function.\\n    f = lambda *args: 0\\n    assert_raises(ValueError, make_scorer, f, needs_threshold=True,\\n                  needs_proba=True)\\n\\n\\ndef test_classification_scores():\\n    # Test classification scorers.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LinearSVC(random_state=0)\\n    clf.fit(X_train, y_train)\\n\\n    for prefix, metric in [(\\'f1\\', f1_score), (\\'precision\\', precision_score),\\n                           (\\'recall\\', recall_score)]:\\n\\n        score1 = get_scorer(\\'%s_weighted\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'weighted\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s_macro\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'macro\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s_micro\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'micro\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=1)\\n        assert_almost_equal(score1, score2)\\n\\n    # test fbeta score that takes an argument\\n    scorer = make_scorer(fbeta_score, beta=2)\\n    score1 = scorer(clf, X_test, y_test)\\n    score2 = fbeta_score(y_test, clf.predict(X_test), beta=2)\\n    assert_almost_equal(score1, score2)\\n\\n    # test that custom scorer can be pickled\\n    unpickled_scorer = pickle.loads(pickle.dumps(scorer))\\n    score3 = unpickled_scorer(clf, X_test, y_test)\\n    assert_almost_equal(score1, score3)\\n\\n    # smoke test the repr:\\n    repr(fbeta_score)\\n\\n\\ndef test_regression_scorers():\\n    # Test regression scorers.\\n    diabetes = load_diabetes()\\n    X, y = diabetes.data, diabetes.target\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = Ridge()\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'r2\\')(clf, X_test, y_test)\\n    score2 = r2_score(y_test, clf.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\ndef test_thresholded_scorers():\\n    # Test scorers that take thresholds.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LogisticRegression(random_state=0)\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\\n    assert_almost_equal(score1, score2)\\n    assert_almost_equal(score1, score3)\\n\\n    logscore = get_scorer(\\'neg_log_loss\\')(clf, X_test, y_test)\\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\\n    assert_almost_equal(-logscore, logloss)\\n\\n    # same for an estimator without decision_function\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\\n    assert_almost_equal(score1, score2)\\n\\n    # test with a regressor (no decision_function)\\n    reg = DecisionTreeRegressor()\\n    reg.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(reg, X_test, y_test)\\n    score2 = roc_auc_score(y_test, reg.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n    # Test that an exception is raised on more than two classes\\n    X, y = make_blobs(random_state=0, centers=3)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf.fit(X_train, y_train)\\n    assert_raises(ValueError, get_scorer(\\'roc_auc\\'), clf, X_test, y_test)\\n\\n\\ndef test_thresholded_scorers_multilabel_indicator_data():\\n    # Test that the scorer work with multilabel-indicator format\\n    # for multilabel and multi-output multi-class classifier\\n    X, y = make_multilabel_classification(allow_unlabeled=False,\\n                                          random_state=0)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n    # Multi-output multi-class predict_proba\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    y_proba = clf.predict_proba(X_test)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, np.vstack(p[:, -1] for p in y_proba).T)\\n    assert_almost_equal(score1, score2)\\n\\n    # Multi-output multi-class decision_function\\n    # TODO Is there any yet?\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    clf._predict_proba = clf.predict_proba\\n    clf.predict_proba = None\\n    clf.decision_function = lambda X: [p[:, 1] for p in clf._predict_proba(X)]\\n\\n    y_proba = clf.decision_function(X_test)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, np.vstack(p for p in y_proba).T)\\n    assert_almost_equal(score1, score2)\\n\\n    # Multilabel predict_proba\\n    clf = OneVsRestClassifier(DecisionTreeClassifier())\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n    # Multilabel decision function\\n    clf = OneVsRestClassifier(LinearSVC(random_state=0))\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\ndef test_unsupervised_scorers():\\n    # Test clustering scorers against gold standard labeling.\\n    # We don\\'t have any real unsupervised Scorers yet.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    km = KMeans(n_clusters=3)\\n    km.fit(X_train)\\n    score1 = get_scorer(\\'adjusted_rand_score\\')(km, X_test, y_test)\\n    score2 = adjusted_rand_score(y_test, km.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\n@ignore_warnings\\ndef test_raises_on_score_list():\\n    # Test that when a list of scores is returned, we raise proper errors.\\n    X, y = make_blobs(random_state=0)\\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\\n    clf = DecisionTreeClassifier()\\n    assert_raises(ValueError, cross_val_score, clf, X, y,\\n                  scoring=f1_scorer_no_average)\\n    grid_search = GridSearchCV(clf, scoring=f1_scorer_no_average,\\n                               param_grid={\\'max_depth\\': [1, 2]})\\n    assert_raises(ValueError, grid_search.fit, X, y)\\n\\n\\n@ignore_warnings\\ndef test_scorer_sample_weight():\\n    # Test that scorers support sample_weight or raise sensible errors\\n\\n    # Unlike the metrics invariance test, in the scorer case it\\'s harder\\n    # to ensure that, on the classifier output, weighted and unweighted\\n    # scores really should be unequal.\\n    X, y = make_classification(random_state=0)\\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\\n                                             random_state=0)\\n    split = train_test_split(X, y, y_ml, random_state=0)\\n    X_train, X_test, y_train, y_test, y_ml_train, y_ml_test = split\\n\\n    sample_weight = np.ones_like(y_test)\\n    sample_weight[:10] = 0\\n\\n    # get sensible estimators for each metric\\n    estimator = _make_estimators(X_train, y_train, y_ml_train)\\n\\n    for name, scorer in SCORERS.items():\\n        if name in MULTILABEL_ONLY_SCORERS:\\n            target = y_ml_test\\n        else:\\n            target = y_test\\n        try:\\n            weighted = scorer(estimator[name], X_test, target,\\n                              sample_weight=sample_weight)\\n            ignored = scorer(estimator[name], X_test[10:], target[10:])\\n            unweighted = scorer(estimator[name], X_test, target)\\n            assert_not_equal(weighted, unweighted,\\n                             msg=\"scorer {0} behaves identically when \"\\n                             \"called with sample weights: {1} vs \"\\n                             \"{2}\".format(name, weighted, unweighted))\\n            assert_almost_equal(weighted, ignored,\\n                                err_msg=\"scorer {0} behaves differently when \"\\n                                \"ignoring samples and setting sample_weight to\"\\n                                \" 0: {1} vs {2}\".format(name, weighted,\\n                                                        ignored))\\n\\n        except TypeError as e:\\n            assert_true(\"sample_weight\" in str(e),\\n                        \"scorer {0} raises unhelpful exception when called \"\\n                        \"with sample weights: {1}\".format(name, str(e)))\\n\\n\\n@ignore_warnings  # UndefinedMetricWarning for P / R scores\\ndef check_scorer_memmap(scorer_name):\\n    scorer, estimator = SCORERS[scorer_name], ESTIMATORS[scorer_name]\\n    if scorer_name in MULTILABEL_ONLY_SCORERS:\\n        score = scorer(estimator, X_mm, y_ml_mm)\\n    else:\\n        score = scorer(estimator, X_mm, y_mm)\\n    assert isinstance(score, numbers.Number), scorer_name\\n\\n\\ndef test_scorer_memmap_input():\\n    # Non-regression test for #6147: some score functions would\\n    # return singleton memmap when computed on memmap data instead of scalar\\n    # float values.\\n    for name in SCORERS.keys():\\n        yield check_scorer_memmap, name\\n\\n\\ndef test_deprecated_names():\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LogisticRegression(random_state=0)\\n    clf.fit(X_train, y_train)\\n\\n    for name in (\\'mean_absolute_error\\', \\'mean_squared_error\\',\\n                 \\'median_absolute_error\\', \\'log_loss\\'):\\n        warning_msg = \"Scoring method %s was renamed to\" % name\\n        for scorer in (get_scorer(name), SCORERS[name]):\\n            assert_warns_message(DeprecationWarning,\\n                                 warning_msg,\\n                                 scorer, clf, X, y)\\n\\n        assert_warns_message(DeprecationWarning,\\n                             warning_msg,\\n                             cross_val_score, clf, X, y, scoring=name)\\n\\n\\ndef test_scoring_is_not_metric():\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         LogisticRegression(), f1_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         LogisticRegression(), roc_auc_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         Ridge(), r2_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         KMeans(), adjusted_rand_score)\\n', '\\nfrom .initializations import initialize_log_assignements,initialize_mcw\\nfrom .base import _full_covariance_matrices\\nfrom .base import _log_normal_matrix\\nfrom .base import BaseMixture\\nfrom .base import _log_B\\n\\nimport numpy as np\\nfrom scipy.special import psi,betaln\\nfrom scipy.misc import logsumexp\\n\\nclass DPVariationalGaussianMixture(BaseMixture):\\n\\n    \"\"\"\\n    Variational Bayesian Estimation of a Gaussian Mixture with Dirichlet Process\\n    \\n    This class allows to infer an approximate posterior distribution over the\\n    parameters of a Gaussian mixture distribution.\\n    \\n    The weights distribution follows a Dirichlet Process with attribute alpha.\\n    \\n    Parameters\\n    ----------\\n    \\n    n_components : int, defaults to 1.\\n        Number of clusters used.\\n    \\n    init : str, defaults to \\'kmeans\\'.\\n        Method used in order to perform the initialization,\\n        must be in [\\'random\\', \\'plus\\', \\'AF_KMC\\', \\'kmeans\\', \\'GMM\\', \\'VBGMM\\'].\\n\\n    reg_covar : float, defaults to 1e-6\\n        In order to avoid null covariances this float is added to the diagonal\\n        of covariance matrices.\\n    \\n    type_init : str, defaults to \\'resp\\'.        \\n        The algorithm is initialized using this data (responsibilities if \\'resp\\'\\n        or means, covariances and weights if \\'mcw\\').\\n\\n    Other parameters\\n    ----------------                \\n    \\n    alpha_0 : float, Optional | defaults to None.\\n        The prior parameter on the weight distribution (Beta).\\n        A high value of alpha_0 will lead to equal weights, while a low value\\n        will allow some clusters to shrink and disappear. Must be greater than 0.\\n    \\n        If None, the value is set to 1/n_components                         \\n    \\n    beta_0 : float, Optional | defaults to None.\\n        The precision prior on the mean distribution (Gaussian).\\n        Must be greater than 0.\\n    \\n        If None, the value is set to 1.0                         \\n    \\n    nu_0 : float, Optional | defaults to None.\\n        The prior of the number of degrees of freedom on the covariance\\n        distributions (Wishart). Must be greater or equal to dim.\\n    \\n        If None, the value is set to dim\\n        \\n    means_prior : array (dim,), Optional | defaults to None\\n        The prior value to compute the value of the means.\\n        \\n        If None, the value is set to the mean of points_data\\n        \\n    cov_wishart_prior : type depends on covariance_type, Optional | defaults to None\\n        If covariance_type is \\'full\\' type must be array (dim,dim)\\n        If covariance_type is \\'spherical\\' type must be float\\n        The prior value to compute the value of the precisions.\\n        \\n    pypcoeff : float | defaults to 0\\n        If 0 the weights are generated according to a Dirichlet Process\\n        If >0 and <=1 the weights are generated according to a Pitman-Yor\\n        Process.\\n\\n    Attributes\\n    ----------\\n    \\n    name : str\\n        The name of the method : \\'VBGMM\\'\\n    \\n    alpha : array of floats (n_components,2)\\n        Contains the parameters of the weight distribution (Beta)\\n    \\n    beta : array of floats (n_components,)\\n        Contains coefficients which are multipied with the precision matrices\\n        to form the precision matrix on the Gaussian distribution of the means.    \\n    \\n    nu : array of floats (n_components,)\\n        Contains the number of degrees of freedom on the distribution of\\n        covariance matrices.\\n    \\n    _inv_prec : array of floats (n_components,dim,dim)\\n        Contains the equivalent of the matrix W described in Bishop\\'s book. It\\n        is proportional to cov.\\n    \\n    _log_det_inv_prec : array of floats (n_components,)\\n        Contains the logarithm of the determinant of W matrices.\\n    \\n    cov : array of floats (n_components,dim,dim)\\n        Contains the computed covariance matrices of the mixture.\\n    \\n    means : array of floats (n_components,dim)\\n        Contains the computed means of the mixture.\\n    \\n    log_weights : array of floats (n_components,)\\n        Contains the logarithm of weights of each cluster.\\n    \\n    iter : int\\n        The number of iterations computed with the method fit()\\n        \\n    convergence_criterion_data : array of floats (iter,)\\n        Stores the value of the convergence criterion computed with data\\n        on which the model is fitted.\\n    \\n    convergence_criterion_test : array of floats (iter,) | if _early_stopping only\\n        Stores the value of the convergence criterion computed with test data\\n        if it exists.\\n    \\n    _is_initialized : bool\\n        Ensures that the method _initialize() has been used before using other\\n        methods such as score() or predict_log_assignements().\\n    \\n    Raises\\n    ------\\n    ValueError : if the parameters are inconsistent, for example if the cluster number is negative, init_type is not in [\\'resp\\',\\'mcw\\']...\\n    \\n    References\\n    ----------\\n    \\'Variational Inference for Dirichlet Process Mixtures\\', D. Blei and M. Jordan\\n \\n    \"\"\"\\n    \\n    def __init__(self, n_components=1,init=\"kmeans\",alpha_0=None,beta_0=None,\\n                 nu_0=None,means_prior=None,cov_wishart_prior=None,\\n                 reg_covar=1e-6,type_init=\\'resp\\',n_jobs=1,pypcoeff=0,\\n                 boost=None):\\n        \\n        super(DPVariationalGaussianMixture, self).__init__()\\n        \\n        self.n_components = n_components\\n        self.covariance_type = \"full\"\\n        self.init = init\\n        self.type_init = type_init\\n        self.reg_covar = reg_covar\\n        self.boost = boost\\n        \\n        self.alpha_0 = alpha_0\\n        self.beta_0 = beta_0\\n        self.nu_0 = nu_0\\n        self.pypcoeff = pypcoeff\\n        self._means_prior = means_prior\\n        self._inv_prec_prior = cov_wishart_prior\\n        self.n_jobs = n_jobs\\n        \\n        self._is_initialized = False\\n        self.iter = 0\\n        self.convergence_criterion_data = []\\n        self.convergence_criterion_test = []\\n        \\n        self._check_common_parameters()\\n        self._check_parameters()\\n        \\n        if pypcoeff==0:\\n            self.name = \\'DPGMM\\'\\n        else:\\n            self.name = \\'PYPGMM\\'\\n\\n    def _check_parameters(self):\\n        \"\"\"\\n        Check the value of the init parameter\\n        \\n        \"\"\"\\n        \\n        if self.init not in [\\'random\\', \\'random_sk\\', \\'plus\\', \\'kmeans\\', \\'AF_KMC\\', \\'GMM\\', \\'VBGMM\\']:\\n            raise ValueError(\"Invalid value for \\'init\\': %s \"\\n                             \"\\'init\\' should be in \"\\n                             \"[\\'random\\',\\'plus\\',\\'kmeans\\',\\'AF_KMC\\',\\'GMM\\',\\'VBGMM\\']\"\\n                             % self.init)\\n            \\n        if self.pypcoeff < 0 or self.pypcoeff > 1:\\n            raise ValueError(\"Invalid value for \\'pypcoeff\\': %s \"\\n                             \"\\'pypcoeff\\' should be between 0 and 1\"\\n                             % self.init)\\n            \\n        if self.boost is not None :\\n            if self.boost < 0:\\n                raise ValueError(\"Invalid value for \\'boost\\': %s \"\\n                             \"\\'boost\\' should be positive\"\\n                             % self.init)\\n            \\n        if self.init == \\'random_sk\\' and self.type_init==\\'mcw\\':\\n            raise ValueError(\"random_sk is only compatible with\"\\n                             \"type_init = resp\")\\n                \\n          \\n    def _initialize(self,points_data,points_test=None):\\n        \"\"\"\\n        This method initializes the Variational Gaussian Mixture by setting the values\\n        of the means, the covariances and other specific parameters (alpha, beta, nu)\\n        \\n        Parameters\\n        ----------\\n        points_data : an array (n_points,dim)\\n            Data on which the model is fitted.\\n        points_test: an array (n_points,dim) | Optional\\n            Data used to do early stopping (avoid overfitting)\\n            \\n        \"\"\"\\n        \\n        n_points,dim = points_data.shape\\n\\n        self._check_prior_parameters(points_data)\\n\\t\\t\\n        if self.type_init == \\'resp\\':\\n            log_assignements = initialize_log_assignements(self.init,self.n_components,points_data,\\n                                                           points_test)\\n            self._inv_prec = np.empty((self.n_components,dim,dim))\\n            self._log_det_inv_prec = np.empty(self.n_components)\\n            self.cov = np.empty((self.n_components,dim,dim))\\n            self.alpha = np.empty((self.n_components,2))\\n            self.log_weights = np.empty(self.n_components)\\n            self._step_M(points_data,log_assignements)\\n            \\n            # Boosting covariance matrices\\n            if self.boost is not None:\\n                self.cov *= self.boost\\n                self._inv_prec *= self.boost\\n                self._log_det_inv_prec += dim * np.log(self.boost)\\n        \\n        elif self.type_init == \\'mcw\\':\\n            #Means, covariances and weights\\n            means,cov,log_weights = initialize_mcw(self.init,self.n_components,points_data,\\n                                                   points_test)\\n            self.cov = cov\\n            self.means = means\\n            self.log_weights = log_weights\\n            \\n            # Hyper parameters\\n            N = np.exp(log_weights) * n_points\\n            self.alpha = np.asarray([1 + N,\\n                          self.alpha_0 + np.hstack((np.cumsum(N[::-1])[-2::-1], 0))]).T\\n            self.alpha += np.asarray([-self.pypcoeff * np.ones(self.n_components),\\n                                      self.pypcoeff * np.arange(self.n_components)]).T\\n            self.beta = self.beta_0 + N\\n            self.nu = self.nu_0 + N\\n            \\n            # Matrix W\\n            self._inv_prec = cov * self.nu[:,np.newaxis,np.newaxis]\\n            self._log_det_inv_prec = np.log(np.linalg.det(self._inv_prec))\\n            \\n        elif self.init == \\'user\\':\\n            \\n            if self.type_init==\\'kmeans\\':\\n                self._initialize_cov(points_data)\\n            \\n            # Hyper parameters\\n            N = np.exp(self.log_weights) * n_points\\n            self.alpha = np.asarray([1 + N,\\n                          self.alpha_0 + np.hstack((np.cumsum(N[::-1])[-2::-1], 0)) ]).T\\n            self.alpha += np.asarray([-self.pypcoeff * np.ones(self.n_components),\\n                                      self.pypcoeff * np.arange(self.n_components)]).T\\n            self.beta = self.beta_0 + N\\n            self.nu = self.nu_0 + N\\n            \\n            # Matrix W\\n            self._inv_prec = self.cov * self.nu[:,np.newaxis,np.newaxis]\\n            self._log_det_inv_prec = np.log(np.linalg.det(self._inv_prec))\\n            \\n            \\n        self._is_initialized = True\\n\\n        \\n    def _step_E(self, points):\\n        \"\"\"\\n        In this step the algorithm evaluates the responsibilities of each points in each cluster\\n        \\n        Parameters\\n        ----------\\n        points : an array (n_points,dim)\\n        \\n        Returns\\n        -------\\n        log_resp: an array (n_points,n_components)\\n            an array containing the logarithm of the responsibilities.\\n        log_prob_norm : an array (n_points,)\\n            logarithm of the probability of each sample in points\\n            \\n        \"\"\"\\n        \\n        n_points,dim = points.shape\\n        \\n        log_gaussian = _log_normal_matrix(points,self.means,self.cov,\\'full\\',self.n_jobs)\\n        log_gaussian -= 0.5 * dim * np.log(self.nu)\\n        digamma_sum = np.sum(psi(.5 * (self.nu - np.arange(0, dim)[:,np.newaxis])),0)\\n        log_lambda = digamma_sum + dim * np.log(2)\\n        \\n        log_prob = self.log_weights + log_gaussian + 0.5 * (log_lambda - dim / self.beta)\\n        \\n        log_prob_norm = logsumexp(log_prob, axis=1)\\n        log_resp = log_prob - log_prob_norm[:,np.newaxis]\\n                    \\n        return log_prob_norm,log_resp\\n    \\n    def _step_M(self,points,log_resp):\\n        \"\"\"\\n        In this step the algorithm updates the values of the parameters (means, covariances,\\n        alpha, beta, nu).\\n        \\n        Parameters\\n        ----------\\n        points : an array (n_points,dim)\\n        \\n        log_resp: an array (n_points,n_components)\\n            an array containing the logarithm of the responsibilities.\\n            \\n        \"\"\"\\n        \\n        n_points,dim = points.shape\\n        \\n        resp = np.exp(log_resp)\\n        \\n        # Convenient statistics\\n        N = np.sum(resp,axis=0) + 10*np.finfo(resp.dtype).eps            #Array (n_components,)\\n        X_barre = 1/N[:,np.newaxis] * np.dot(resp.T,points)              #Array (n_components,dim)\\n        S = _full_covariance_matrices(points,X_barre,N,resp,self.reg_covar,self.n_jobs)\\n        \\n        #Parameters update\\n        self.alpha = np.asarray([1.0 + N,\\n                                  self.alpha_0 + np.hstack((np.cumsum(N[::-1])[-2::-1], 0))]).T\\n        self.alpha += np.asarray([-self.pypcoeff * np.ones(self.n_components),\\n                                      self.pypcoeff * np.arange(self.n_components)]).T\\n        self.beta = self.beta_0 + N\\n        self.nu = self.nu_0 + N\\n        \\n        # Weights update\\n        for i in range(self.n_components):\\n            if i==0:\\n                self.log_weights[i] = psi(self.alpha[i][0]) - psi(np.sum(self.alpha[i]))\\n            else:\\n                self.log_weights[i] = psi(self.alpha[i][0]) - psi(np.sum(self.alpha[i]))\\n                self.log_weights[i] += self.log_weights[i-1] + psi(self.alpha[i-1][1]) - psi(self.alpha[i-1][0])\\n        \\n        # Means update\\n        means = self.beta_0 * self._means_prior + N[:,np.newaxis] * X_barre\\n        self.means = means * np.reciprocal(self.beta)[:,np.newaxis]\\n        self.means_estimated = self.means\\n        \\n        # Covariance update\\n        for i in range(self.n_components):\\n            diff = X_barre[i] - self._means_prior\\n            product = self.beta_0 * N[i]/self.beta[i] * np.outer(diff,diff)\\n            self._inv_prec[i] = self._inv_prec_prior + N[i] * S[i] + product\\n            \\n            det_inv_prec = np.linalg.det(self._inv_prec[i])\\n            self._log_det_inv_prec[i] = np.log(det_inv_prec)\\n            self.cov[i] = self._inv_prec[i] / self.nu[i]\\n        \\n    def _convergence_criterion_simplified(self,points,log_resp,log_prob_norm):\\n        \"\"\"\\n        Compute the lower bound of the likelihood using the simplified Blei and\\n        Jordan formula. Can only be used with data which fits the model.\\n        \\n        \\n        Parameters\\n        ----------\\n        points : an array (n_points,dim)\\n        \\n        log_resp: an array (n_points,n_components)\\n            an array containing the logarithm of the responsibilities.\\n            \\n        log_prob_norm : an array (n_points,)\\n            logarithm of the probability of each sample in points\\n        \\n        Returns\\n        -------\\n        result : float\\n            the lower bound of the likelihood\\n            \\n        \"\"\"\\n        \\n        resp = np.exp(log_resp)\\n        n_points,dim = points.shape\\n        \\n        prec = np.linalg.inv(self._inv_prec)\\n        prec_prior = np.linalg.inv(self._inv_prec_prior)\\n        \\n        lower_bound = np.zeros(self.n_components)\\n        \\n        for i in range(self.n_components):\\n            \\n            lower_bound[i] = _log_B(prec_prior,self.nu_0) - _log_B(prec[i],self.nu[i])\\n            \\n            resp_i = resp[:,i:i+1]\\n            log_resp_i = log_resp[:,i:i+1]\\n            \\n            lower_bound[i] -= np.sum(resp_i*log_resp_i)\\n            lower_bound[i] += dim*0.5*(np.log(self.beta_0) - np.log(self.beta[i]))\\n        \\n        result = np.sum(lower_bound)\\n        result -= self.n_components * betaln(1,self.alpha_0)\\n        result += np.sum(betaln(self.alpha.T[0],self.alpha.T[1]))\\n        result -= n_points * dim * 0.5 * np.log(2*np.pi)\\n        \\n        return result\\n    \\n    \\n    def _convergence_criterion(self,points,log_resp,log_prob_norm):\\n        \"\"\"\\n        Compute the lower bound of the likelihood using the Blei and Jordan formula.\\n        The formula cannot be simplified (as it is done in scikit-learn) as we also\\n        use it to calculate the lower bound of test points, in this case no\\n        simplification can be done.\\n          \\n        \\n        Parameters\\n        ----------\\n        points : an array (n_points,dim)\\n        \\n        log_resp: an array (n_points,n_components)\\n            an array containing the logarithm of the responsibilities.\\n            \\n        log_prob_norm : an array (n_points,)\\n            logarithm of the probability of each sample in points\\n        \\n        Returns\\n        -------\\n        result : float\\n            the lower bound of the likelihood\\n            \\n        \"\"\"\\n        \\n        resp = np.exp(log_resp)\\n        n_points,dim = points.shape\\n        \\n        # Convenient statistics\\n        N = np.sum(resp,axis=0) + 10*np.finfo(resp.dtype).eps               #Array (n_components,)\\n        X_barre = 1/N[:,np.newaxis] * np.dot(resp.T,points)                 #Array (n_components,dim)\\n        S = _full_covariance_matrices(points,X_barre,N,resp,self.reg_covar,self.n_jobs)\\n             \\n        prec = np.linalg.inv(self._inv_prec)\\n        prec_prior = np.linalg.inv(self._inv_prec_prior)\\n        \\n        lower_bound = np.zeros(self.n_components)\\n        \\n        for i in range(self.n_components):\\n            \\n            digamma_sum = np.sum(psi(.5 * (self.nu[i] - np.arange(0, dim)[:,np.newaxis])),0)\\n            log_det_prec_i = digamma_sum + dim * np.log(2) - self._log_det_inv_prec[i] #/!\\\\ Inverse\\n            \\n            #First line\\n            lower_bound[i] = log_det_prec_i - dim/self.beta[i] - self.nu[i]*np.trace(np.dot(S[i],prec[i]))\\n            diff = X_barre[i] - self.means[i]\\n            lower_bound[i] += -self.nu[i]*np.dot(diff,np.dot(prec[i],diff.T))\\n            lower_bound[i] *= 0.5 * N[i]\\n            \\n            #Second line\\n            lower_bound[i] += _log_B(prec_prior,self.nu_0) - _log_B(prec[i],self.nu[i])\\n            \\n            resp_i = resp[:,i:i+1]\\n            log_resp_i = log_resp[:,i:i+1]\\n            \\n            lower_bound[i] -= np.sum(resp_i*log_resp_i)\\n            lower_bound[i] += 0.5 * (self.nu_0 - self.nu[i]) * log_det_prec_i\\n            lower_bound[i] += dim*0.5*(np.log(self.beta_0) - np.log(self.beta[i]))\\n            lower_bound[i] += dim*0.5*(1 - self.beta_0/self.beta[i] + self.nu[i])\\n            \\n            #Third line without the last term which is not summed\\n            diff = self.means[i] - self._means_prior\\n            lower_bound[i] += -0.5*self.beta_0*self.nu[i]*np.dot(diff,np.dot(prec[i],diff.T))\\n            lower_bound[i] += -0.5*self.nu[i]*np.trace(np.dot(self._inv_prec_prior,prec[i]))\\n            \\n            #Terms with alpha\\n            lower_bound[i] += (N[i] + 1.0 - self.alpha[i,0]) * (psi(self.alpha[i,0]) - psi(np.sum(self.alpha[i])))\\n            lower_bound[i] += (np.sum(N[i+1::]) + self.alpha_0 - self.alpha[i,1]) * (psi(self.alpha[i,1]) - psi(np.sum(self.alpha[i])))\\n        \\n        result = np.sum(lower_bound)\\n        result -= self.n_components * betaln(1,self.alpha_0)\\n        result += np.sum(betaln(self.alpha.T[0],self.alpha.T[1]))\\n        result -= n_points * dim * 0.5 * np.log(2*np.pi)\\n        \\n        return result\\n\\n    \\n    def _get_parameters(self):\\n        return (self.log_weights, self.means, self.cov,\\n                self.alpha, self.beta, self.nu)\\n    \\n\\n    def _set_parameters(self, params,verbose=True):\\n        (self.log_weights, self.means, self.cov,\\n        self.alpha, self.beta, self.nu )= params\\n         \\n        # Matrix W\\n        self._inv_prec = self.cov * self.nu[:,np.newaxis,np.newaxis]\\n        self._log_det_inv_prec = np.log(np.linalg.det(self._inv_prec))\\n        if self.n_components != len(self.means) and verbose:\\n            print(\\'The number of components changed\\')\\n        self.n_components = len(self.means)\\n        \\n    def _limiting_model(self,points):\\n        \\n        n_points,dim = points.shape\\n        log_resp = self.predict_log_resp(points)\\n        _,n_components = log_resp.shape\\n    \\n        exist = np.zeros(n_components)\\n        \\n        for i in range(n_points):\\n            for j in range(n_components):\\n                if np.argmax(log_resp[i])==j:\\n                    exist[j] = 1\\n        \\n        idx_existing = np.where(exist==1)\\n        \\n        log_weights = self.log_weights[idx_existing]\\n        means = self.means[idx_existing]\\n        cov = self.cov[idx_existing]\\n        alpha = self.alpha[idx_existing]\\n        beta = self.beta[idx_existing]\\n        nu = self.nu[idx_existing]\\n        \\n        params = (log_weights, means, cov,\\n                  alpha, beta, nu)\\n        \\n        return params', \"\\nfrom pathlib import Path\\nimport textwrap\\n\\nfrom django.core.management.base import BaseCommand\\n\\n\\nclass Command(BaseCommand):\\n    help = 'Convert a template to use Fluent for l10n'\\n    requires_system_checks = False\\n\\n    def add_arguments(self, parser):\\n        subparsers = parser.add_subparsers(\\n            title='subcommand', dest='subcommand'\\n        )\\n        subparsers.add_parser('help')\\n\\n        recipe_parser = subparsers.add_parser(\\n            'recipe',\\n            description='Create migration recipe from template'\\n        )\\n        recipe_parser.add_argument('template', type=Path)\\n\\n        ftl_parser = subparsers.add_parser(\\n            'ftl',\\n            description='Create Fluent file with existing recipe'\\n        )\\n        ftl_parser.add_argument(\\n            'recipe_or_template', type=Path,\\n            help='Path to the recipe or the template from which the recipe was generated'\\n        )\\n        ftl_parser.add_argument(\\n            'locales', nargs='*', default=['en'], metavar='ab-CD',\\n            help='Locale codes to create ftl files for'\\n        )\\n\\n        template_parser = subparsers.add_parser(\\n            'template',\\n            description='Create template_ftl.html file with existing recipe'\\n        )\\n        template_parser.add_argument('template', type=Path)\\n\\n        activation_parser = subparsers.add_parser(\\n            'activation',\\n            description='Port activation data from .lang for a recipe/template'\\n        )\\n        activation_parser.add_argument(\\n            'recipe_or_template', type=Path,\\n            help='Path to the recipe or the template from which the recipe was generated'\\n        )\\n\\n    def handle(self, subcommand, **kwargs):\\n        if subcommand == 'recipe':\\n            return self.create_recipe(**kwargs)\\n        if subcommand == 'ftl':\\n            return self.create_ftl(**kwargs)\\n        if subcommand == 'template':\\n            return self.create_template(**kwargs)\\n        if subcommand == 'activation':\\n            return self.activation(**kwargs)\\n        return self.handle_help(**kwargs)\\n\\n    def handle_help(self, **kwargs):\\n        self.stdout.write(textwrap.dedent('''\\\\\\n            To migrate a template from .lang to Fluent, use the subcommands like so\\n\\n            ./manage.py fluent recipe bedrock/app/templates/app/some.html\\n\\n            # edit IDs in lib/fluent_migrations/app/some.py\\n\\n            ./manage.py fluent template bedrock/app/templates/app/some.html\\n            ./manage.py fluent ftl bedrock/app/templates/app/some.html\\n\\n            More documentation on https://bedrock.readthedocs.io/en/latest/fluent-conversion.html.\\n        '''))\\n\\n    def create_recipe(self, template, **kwargs):\\n        from ._fluent_recipe import Recipe\\n        recipe = Recipe(self)\\n        recipe.handle(template)\\n\\n    def create_template(self, template, **kwargs):\\n        from ._fluent_templater import Templater\\n        templater = Templater(self)\\n        templater.handle(template)\\n\\n    def create_ftl(self, recipe_or_template, locales, **kwargs):\\n        from ._fluent_ftl import FTLCreator\\n        ftl_creator = FTLCreator(self)\\n        for locale in locales:\\n            ftl_creator.handle(recipe_or_template, locale)\\n\\n    def activation(self, recipe_or_template, **kwargs):\\n        from ._fluent_activation import Activation\\n        activation = Activation(self)\\n        activation.handle(recipe_or_template)\\n\", 'from test.support import run_unittest\\nfrom test.script_helper import assert_python_failure, temp_dir\\nimport unittest\\nimport sys\\nimport cgitb\\n\\nclass TestCgitb(unittest.TestCase):\\n\\n    def test_fonts(self):\\n        text = \"Hello Robbie!\"\\n        self.assertEqual(cgitb.small(text), \"<small>{}</small>\".format(text))\\n        self.assertEqual(cgitb.strong(text), \"<strong>{}</strong>\".format(text))\\n        self.assertEqual(cgitb.grey(text),\\n                         \\'<font color=\"#909090\">{}</font>\\'.format(text))\\n\\n    def test_blanks(self):\\n        self.assertEqual(cgitb.small(\"\"), \"\")\\n        self.assertEqual(cgitb.strong(\"\"), \"\")\\n        self.assertEqual(cgitb.grey(\"\"), \"\")\\n\\n    def test_html(self):\\n        try:\\n            raise ValueError(\"Hello World\")\\n        except ValueError as err:\\n            # If the html was templated we could do a bit more here.\\n            # At least check that we get details on what we just raised.\\n            html = cgitb.html(sys.exc_info())\\n            self.assertIn(\"ValueError\", html)\\n            self.assertIn(str(err), html)\\n\\n    def test_text(self):\\n        try:\\n            raise ValueError(\"Hello World\")\\n        except ValueError as err:\\n            text = cgitb.text(sys.exc_info())\\n            self.assertIn(\"ValueError\", text)\\n            self.assertIn(\"Hello World\", text)\\n\\n    def test_syshook_no_logdir_default_format(self):\\n        with temp_dir() as tracedir:\\n            rc, out, err = assert_python_failure(\\n                  \\'-c\\',\\n                  (\\'import cgitb; cgitb.enable(logdir=%s); \\'\\n                   \\'raise ValueError(\"Hello World\")\\') % repr(tracedir))\\n        out = out.decode(sys.getfilesystemencoding())\\n        self.assertIn(\"ValueError\", out)\\n        self.assertIn(\"Hello World\", out)\\n        # By default we emit HTML markup.\\n        self.assertIn(\\'<p>\\', out)\\n        self.assertIn(\\'</p>\\', out)\\n\\n    def test_syshook_no_logdir_text_format(self):\\n        # Issue 12890: we were emitting the <p> tag in text mode.\\n        with temp_dir() as tracedir:\\n            rc, out, err = assert_python_failure(\\n                  \\'-c\\',\\n                  (\\'import cgitb; cgitb.enable(format=\"text\", logdir=%s); \\'\\n                   \\'raise ValueError(\"Hello World\")\\') % repr(tracedir))\\n        out = out.decode(sys.getfilesystemencoding())\\n        self.assertIn(\"ValueError\", out)\\n        self.assertIn(\"Hello World\", out)\\n        self.assertNotIn(\\'<p>\\', out)\\n        self.assertNotIn(\\'</p>\\', out)\\n\\n\\ndef test_main():\\n    run_unittest(TestCgitb)\\n\\nif __name__ == \"__main__\":\\n    test_main()\\n', '\"\"\"\\nThis module contains helper classes and methods\\nfor the facebook integration module\\n\\n.. module:: application.facebook.facebook\\n\\n.. moduleauthor:: Devin Schwab <dts34@case.edu>\\n\"\"\"\\n\\nimport facebooksdk as fb\\nimport models\\n\\nfrom flask import flash\\n\\nclass AlbumList(object):\\n    def __init__(self, token):\\n        \"\"\"\\n        Given an an access token this class\\n        will get all albums for the object associated with the token\\n        (i.e. a page or a user)\\n\\n        It will lazily construct an Album instance for each of\\n        the album ids returned\\n        \"\"\"\\n        \\n        self.graph = fb.GraphAPI(token.access_token)\\n        albums_data = self.graph.get_connections(\\'me\\', \\'albums\\')[\\'data\\']\\n\\n        self.album_ids = {}\\n        self.album_names = {}\\n        for data in albums_data:\\n            self.album_ids[data[\\'id\\']] = data\\n            self.album_names[data[\\'name\\']] = data\\n\\n    def get_albums_by_name(self, names):\\n        \"\"\"\\n        Given a list of names this method will\\n        return album objects for each matching name.\\n\\n        If a name is not found then it is silently ignored.\\n\\n        This method returns a dictionary mapping name\\n        to Album object.\\n        \"\"\"\\n\\n        albums = {}\\n        for name in names:\\n            if name in self.album_names:\\n                if isinstance(self.album_names[name], Album):\\n                    albums[name] = self.album_names[name]\\n                else:\\n                    self.album_names[name] = Album(graph=self.graph,\\n                                                   album_data=self.album_names[name])\\n                    self.album_ids[self.album_names[name].me] = self.album_names[name]\\n                    albums[name] = self.album_names[name]\\n        return albums\\n\\n    def get_albums_by_id(self, ids):\\n        \"\"\"\\n        Given a list of ids this method will\\n        return album objects for each matching id.\\n\\n        If an id is not found then it is silently ignored.\\n\\n        This method returns a dictionary mapping id to\\n        Album object\\n        \"\"\"\\n\\n        albums = {}\\n        for album_id in ids:\\n            if album_id in self.album_ids:\\n                if isinstance(self.album_ids[album_id], Album):\\n                    albums[album_id] = self.album_ids[album_id]\\n                else:\\n                    self.album_ids[album_id] = Album(graph=self.graph,\\n                                                     album_data=self.album_ids[album_id])\\n                    self.album_names[self.album_ids[album_id].name] = self.album_ids[album_id]\\n                    albums[album_id] = self.album_ids[album_id]\\n        return albums\\n        \\n\\n    def get_all_albums_by_id(self):\\n        \"\"\"\\n        This method returns a dictionary of all\\n        albums with album ids as the keys\\n        \"\"\"\\n\\n        for album_id in self.album_ids:\\n            if not isinstance(self.album_ids[album_id], Album):\\n                self.album_ids[album_id] = Album(graph=self.graph,\\n                                                 album_data=self.album_ids[album_id])\\n                self.album_names[self.album_ids[album_id].name] = self.album_ids[album_id]\\n\\n        return self.album_ids\\n\\n    def get_all_albums_by_name(self):\\n        \"\"\"\\n        This method returns a dictionary of all\\n        albums with album names as the keys\\n        \"\"\"\\n\\n        for name in self.album_names:\\n            if not isinstance(self.album_names[name], Album):\\n                self.album_names[name] = Album(graph=self.graph,\\n                                               album_data=self.album_names[name])\\n                self.album_ids[self.album_names[name].me] = self.album_names[name]\\n\\n        return self.album_names\\n                \\n        \\nclass Album(object):\\n    def __init__(self, graph=None, token=None, album_id=None, album_data=None):\\n        \"\"\"\\n        Initializes a new Album object.\\n\\n        If graph is provided then the graph object is saved to this\\n        instance.\\n\\n        If the token is provided then the graph object for this token\\n        is created and saved to this instance.\\n\\n        If both are none then an error is raised.\\n\\n        If album_id is provided then the graph object is queried\\n        for the id and the album object populates itself with this data\\n\\n        If album_data is provided then the graph object is populated\\n        with the data in the json derived object\\n\\n        If both are None then an error is raised\\n        \"\"\"\\n\\n        if graph is None and token is None:\\n            raise TypeError(\"Either a graph object must be provided or a token must be provided\")\\n\\n        if graph is not None:\\n            self.graph = graph\\n            query = models.AccessTokenModel.all()\\n            query.filter(\\'access_token =\\', graph.access_token)\\n\\n            try:\\n                self.token = query.fetch(1)[0]\\n            except IndexError:\\n                raise TypeError(\\'The token object provided was not an AccessTokenModel instance\\')\\n        else:\\n            self.graph = fb.GraphAPI(token.access_token)\\n            self.token = token\\n\\n        if album_id is None and album_data is None:\\n            raise TypeError(\"Either an album id or a album data must be provided\")\\n\\n        if album_id is not None:\\n            album_data = self.graph.get_object(album_id)\\n\\n        self.me = album_data[\\'id\\']\\n        self.name = album_data[\\'name\\']\\n        self.desc = album_data.get(\\'description\\', None)\\n        self.count = album_data.get(\\'count\\', 0)\\n        if \\'cover_photo\\' in album_data:\\n            self.cover_photo = Photo(self.me, graph=self.graph, photo_id=album_data[\\'cover_photo\\']).thumbnail\\n        else:\\n            self.cover_photo = None\\n            \\n    def get_model(self):\\n        query = models.AlbumModel.all()\\n        query.filter(\\'me =\\', self.me)\\n\\n        try:\\n            return  query.fetch(1)[0]\\n        except IndexError:\\n            cover_thumb = None\\n            if self.cover_photo is not None:\\n                cover_thumb = self.cover_photo\\n\\n            entity = models.AlbumModel(me=self.me,\\n                                       token=self.token,\\n                                       name=self.name,\\n                                       desc=self.desc,\\n                                       cover_photo=cover_thumb)\\n            entity.put()\\n            return entity\\n\\n    def get_photos(self):\\n        \"\"\"\\n        Get a list of Photo objects\\n        \"\"\"\\n\\n        photos_data = self.graph.get_connections(self.me, \\'photos\\')[\\'data\\']\\n        \\n        photos = []\\n        for photo_data in photos_data:\\n            query = models.PhotoModel.all()\\n            query.filter(\\'me =\\', photo_data[\\'id\\'])\\n            try:\\n                photos.append(query.fetch(1)[0])\\n            except IndexError:\\n                name = None\\n                if \\'name\\' in photo_data:\\n                    name = photo_data[\\'name\\']\\n\\n                orig = photo_data[\\'images\\'][0][\\'source\\']\\n            \\n                entity = models.PhotoModel(me=photo_data[\\'id\\'],\\n                                           album_id=self.me,\\n                                           name=name,\\n                                           thumbnail=photo_data[\\'picture\\'],\\n                                           original=orig)\\n                entity.put()\\n\\n                photos.append(entity)\\n        \\n        return photos\\n            \\nclass Photo(object):\\n    def __init__(self, album_id, graph=None, token=None, photo_id=None, photo_data=None):\\n        if graph is None and token is None:\\n            raise TypeError(\"Either a graph object must be provided or a token must be provided\")\\n\\n        if graph is not None:\\n            self.graph = graph\\n        else:\\n            self.graph = fb.GraphAPI(token.access_token)\\n\\n        if photo_id is None and photo_data is None:\\n            raise TypeError(\"Either an album id or a album data must be provided\")\\n\\n        if photo_id is not None:\\n            photo_data = self.graph.get_object(photo_id)\\n\\n        self.me = photo_data[\\'id\\']\\n        self.name = photo_data.get(\\'name\\', None)\\n        self.thumbnail = photo_data[\\'picture\\']\\n        self.original = photo_data[\\'images\\'][0][\\'source\\']\\n        self.album_id = album_id\\n\\n    def get_model(self):\\n        query = models.PhotoModel.all()\\n        query.filter(\\'me =\\', self.me)\\n\\n        try:\\n            return query.fetch(1)[0]\\n        except IndexError:\\n            entity = models.PhotoModel(me=self.me,\\n                                       album_id=self.album_id,\\n                                       name=self.name,\\n                                       thumbnail=self.thumbnail,\\n                                       original=self.original)\\n            entity.put()\\n            return entity\\n            ', \"\\nfrom ibis.common import IbisError\\nimport ibis.expr.operations as ops\\nimport ibis.expr.types as ir\\nimport ibis.expr.temporal as T\\n\\nfrom ibis.expr.tests.mocks import MockConnection\\nfrom ibis.compat import unittest\\n\\n\\nclass TestFixedOffsets(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.con = MockConnection()\\n        self.table = self.con.table('alltypes')\\n\\n    def test_upconvert(self):\\n        cases = [\\n            (T.day(14), 'w', T.week(2)),\\n            (T.hour(72), 'd', T.day(3)),\\n            (T.minute(240), 'h', T.hour(4)),\\n            (T.second(360), 'm', T.minute(6)),\\n            (T.second(3 * 86400), 'd', T.day(3)),\\n            (T.millisecond(5000), 's', T.second(5)),\\n            (T.microsecond(5000000), 's', T.second(5)),\\n            (T.nanosecond(5000000000), 's', T.second(5)),\\n        ]\\n\\n        for offset, unit, expected in cases:\\n            result = offset.to_unit(unit)\\n            assert result.equals(expected)\\n\\n    def test_multiply(self):\\n        offset = T.day(2)\\n\\n        assert (offset * 2).equals(T.day(4))\\n        assert (offset * (-2)).equals(T.day(-4))\\n        assert (3 * offset).equals(T.day(6))\\n        assert ((-3) * offset).equals(T.day(-6))\\n\\n    def test_repr(self):\\n        assert repr(T.day()) == '<Timedelta: 1 day>'\\n        assert repr(T.day(2)) == '<Timedelta: 2 days>'\\n        assert repr(T.year()) == '<Timedelta: 1 year>'\\n        assert repr(T.month(2)) == '<Timedelta: 2 months>'\\n        assert repr(T.second(40)) == '<Timedelta: 40 seconds>'\\n\\n    def test_cannot_upconvert(self):\\n        cases = [\\n            (T.day(), 'w'),\\n            (T.hour(), 'd'),\\n            (T.minute(), 'h'),\\n            (T.second(), 'm'),\\n            (T.second(), 'd'),\\n            (T.millisecond(), 's'),\\n            (T.microsecond(), 's'),\\n            (T.nanosecond(), 's'),\\n        ]\\n\\n        for delta, target in cases:\\n            self.assertRaises(IbisError, delta.to_unit, target)\\n\\n    def test_downconvert_second_parts(self):\\n        K = 2\\n\\n        sec = T.second(K)\\n        milli = T.millisecond(K)\\n        micro = T.microsecond(K)\\n        nano = T.nanosecond(K)\\n\\n        cases = [\\n            (sec.to_unit('s'), T.second(K)),\\n            (sec.to_unit('ms'), T.millisecond(K * 1000)),\\n            (sec.to_unit('us'), T.microsecond(K * 1000000)),\\n            (sec.to_unit('ns'), T.nanosecond(K * 1000000000)),\\n\\n            (milli.to_unit('ms'), T.millisecond(K)),\\n            (milli.to_unit('us'), T.microsecond(K * 1000)),\\n            (milli.to_unit('ns'), T.nanosecond(K * 1000000)),\\n\\n            (micro.to_unit('us'), T.microsecond(K)),\\n            (micro.to_unit('ns'), T.nanosecond(K * 1000)),\\n\\n            (nano.to_unit('ns'), T.nanosecond(K))\\n        ]\\n        self._check_cases(cases)\\n\\n    def test_downconvert_hours(self):\\n        K = 2\\n        offset = T.hour(K)\\n\\n        cases = [\\n            (offset.to_unit('h'), T.hour(K)),\\n            (offset.to_unit('m'), T.minute(K * 60)),\\n            (offset.to_unit('s'), T.second(K * 3600)),\\n            (offset.to_unit('ms'), T.millisecond(K * 3600000)),\\n            (offset.to_unit('us'), T.microsecond(K * 3600000000)),\\n            (offset.to_unit('ns'), T.nanosecond(K * 3600000000000))\\n        ]\\n        self._check_cases(cases)\\n\\n    def test_downconvert_day(self):\\n        K = 2\\n\\n        week = T.week(K)\\n        day = T.day(K)\\n\\n        cases = [\\n            (week.to_unit('d'), T.day(K * 7)),\\n            (week.to_unit('h'), T.hour(K * 7 * 24)),\\n\\n            (day.to_unit('d'), T.day(K)),\\n            (day.to_unit('h'), T.hour(K * 24)),\\n            (day.to_unit('m'), T.minute(K * 1440)),\\n            (day.to_unit('s'), T.second(K * 86400)),\\n            (day.to_unit('ms'), T.millisecond(K * 86400000)),\\n            (day.to_unit('us'), T.microsecond(K * 86400000000)),\\n            (day.to_unit('ns'), T.nanosecond(K * 86400000000000))\\n        ]\\n        self._check_cases(cases)\\n\\n    def test_combine_with_different_kinds(self):\\n        cases = [\\n            (T.day() + T.minute(), T.minute(1441)),\\n            (T.second() + T.millisecond(10), T.millisecond(1010)),\\n            (T.hour() + T.minute(5) + T.second(10), T.second(3910))\\n        ]\\n        self._check_cases(cases)\\n\\n    def test_timedelta_generic_api(self):\\n        cases = [\\n            (T.timedelta(weeks=2), T.week(2)),\\n            (T.timedelta(days=3), T.day(3)),\\n            (T.timedelta(hours=4), T.hour(4)),\\n            (T.timedelta(minutes=5), T.minute(5)),\\n            (T.timedelta(seconds=6), T.second(6)),\\n            (T.timedelta(milliseconds=7), T.millisecond(7)),\\n            (T.timedelta(microseconds=8), T.microsecond(8)),\\n            (T.timedelta(nanoseconds=9), T.nanosecond(9)),\\n        ]\\n        self._check_cases(cases)\\n\\n    def _check_cases(self, cases):\\n        for x, y in cases:\\n            assert x.equals(y)\\n\\n    def test_offset_timestamp_expr(self):\\n        c = self.table.i\\n        x = T.timedelta(days=1)\\n\\n        expr = x + c\\n        assert isinstance(expr, ir.TimestampArray)\\n        assert isinstance(expr.op(), ops.TimestampDelta)\\n\\n        # test radd\\n        expr = c + x\\n        assert isinstance(expr, ir.TimestampArray)\\n        assert isinstance(expr.op(), ops.TimestampDelta)\\n\\n\\nclass TestTimedelta(unittest.TestCase):\\n\\n    def test_compound_offset(self):\\n        # These are not yet allowed (e.g. 1 month + 1 hour)\\n        pass\\n\\n    def test_offset_months(self):\\n        pass\\n\", 'import sys\\n\\nfrom django import http\\nfrom django.core import signals\\nfrom django.utils.encoding import force_unicode\\nfrom django.utils.importlib import import_module\\n\\nclass BaseHandler(object):\\n    # Changes that are always applied to a response (in this order).\\n    response_fixes = [\\n        http.fix_location_header,\\n        http.conditional_content_removal,\\n        http.fix_IE_for_attach,\\n        http.fix_IE_for_vary,\\n    ]\\n\\n    def __init__(self):\\n        self._request_middleware = self._view_middleware = self._response_middleware = self._exception_middleware = None\\n\\n    def load_middleware(self):\\n        \"\"\"\\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\\n\\n        Must be called after the environment is fixed (see __call__).\\n        \"\"\"\\n        from django.conf import settings\\n        from django.core import exceptions\\n        self._view_middleware = []\\n        self._response_middleware = []\\n        self._exception_middleware = []\\n\\n        request_middleware = []\\n        for middleware_path in settings.MIDDLEWARE_CLASSES:\\n            try:\\n                dot = middleware_path.rindex(\\'.\\')\\n            except ValueError:\\n                raise exceptions.ImproperlyConfigured(\\'%s isn\\\\\\'t a middleware module\\' % middleware_path)\\n            mw_module, mw_classname = middleware_path[:dot], middleware_path[dot+1:]\\n            try:\\n                mod = import_module(mw_module)\\n            except ImportError, e:\\n                raise exceptions.ImproperlyConfigured(\\'Error importing middleware %s: \"%s\"\\' % (mw_module, e))\\n            try:\\n                mw_class = getattr(mod, mw_classname)\\n            except AttributeError:\\n                raise exceptions.ImproperlyConfigured(\\'Middleware module \"%s\" does not define a \"%s\" class\\' % (mw_module, mw_classname))\\n\\n            try:\\n                mw_instance = mw_class()\\n            except exceptions.MiddlewareNotUsed:\\n                continue\\n\\n            if hasattr(mw_instance, \\'process_request\\'):\\n                request_middleware.append(mw_instance.process_request)\\n            if hasattr(mw_instance, \\'process_view\\'):\\n                self._view_middleware.append(mw_instance.process_view)\\n            if hasattr(mw_instance, \\'process_response\\'):\\n                self._response_middleware.insert(0, mw_instance.process_response)\\n            if hasattr(mw_instance, \\'process_exception\\'):\\n                self._exception_middleware.insert(0, mw_instance.process_exception)\\n\\n        # We only assign to this when initialization is complete as it is used\\n        # as a flag for initialization being complete.\\n        self._request_middleware = request_middleware\\n\\n    def get_response(self, request):\\n        \"Returns an HttpResponse object for the given HttpRequest\"\\n        from django.core import exceptions, urlresolvers\\n        from django.conf import settings\\n\\n        try:\\n            try:\\n                # Setup default url resolver for this thread.\\n                urlconf = settings.ROOT_URLCONF\\n                urlresolvers.set_urlconf(urlconf)\\n                resolver = urlresolvers.RegexURLResolver(r\\'^/\\', urlconf)\\n\\n                # Apply request middleware\\n                for middleware_method in self._request_middleware:\\n                    response = middleware_method(request)\\n                    if response:\\n                        return response\\n\\n                if hasattr(request, \"urlconf\"):\\n                    # Reset url resolver with a custom urlconf.\\n                    urlconf = request.urlconf\\n                    urlresolvers.set_urlconf(urlconf)\\n                    resolver = urlresolvers.RegexURLResolver(r\\'^/\\', urlconf)\\n\\n                callback, callback_args, callback_kwargs = resolver.resolve(\\n                        request.path_info)\\n\\n                # Apply view middleware\\n                for middleware_method in self._view_middleware:\\n                    response = middleware_method(request, callback, callback_args, callback_kwargs)\\n                    if response:\\n                        return response\\n\\n                try:\\n                    response = callback(request, *callback_args, **callback_kwargs)\\n                except Exception, e:\\n                    # If the view raised an exception, run it through exception\\n                    # middleware, and if the exception middleware returns a\\n                    # response, use that. Otherwise, reraise the exception.\\n                    for middleware_method in self._exception_middleware:\\n                        response = middleware_method(request, e)\\n                        if response:\\n                            return response\\n                    raise\\n\\n                # Complain if the view returned None (a common error).\\n                if response is None:\\n                    try:\\n                        view_name = callback.func_name # If it\\'s a function\\n                    except AttributeError:\\n                        view_name = callback.__class__.__name__ + \\'.__call__\\' # If it\\'s a class\\n                    raise ValueError(\"The view %s.%s didn\\'t return an HttpResponse object.\" % (callback.__module__, view_name))\\n\\n                return response\\n            except http.Http404, e:\\n                if settings.DEBUG:\\n                    from django.views import debug\\n                    return debug.technical_404_response(request, e)\\n                else:\\n                    try:\\n                        callback, param_dict = resolver.resolve404()\\n                        return callback(request, **param_dict)\\n                    except:\\n                        try:\\n                            return self.handle_uncaught_exception(request, resolver, sys.exc_info())\\n                        finally:\\n                            receivers = signals.got_request_exception.send(sender=self.__class__, request=request)\\n            except exceptions.PermissionDenied:\\n                return http.HttpResponseForbidden(\\'<h1>Permission denied</h1>\\')\\n            except SystemExit:\\n                # Allow sys.exit() to actually exit. See tickets #1023 and #4701\\n                raise\\n            except: # Handle everything else, including SuspiciousOperation, etc.\\n                # Get the exception info now, in case another exception is thrown later.\\n                receivers = signals.got_request_exception.send(sender=self.__class__, request=request)\\n                return self.handle_uncaught_exception(request, resolver, sys.exc_info())\\n        finally:\\n            # Reset URLconf for this thread on the way out for complete\\n            # isolation of request.urlconf\\n            urlresolvers.set_urlconf(None)\\n\\n    def handle_uncaught_exception(self, request, resolver, exc_info):\\n        \"\"\"\\n        Processing for any otherwise uncaught exceptions (those that will\\n        generate HTTP 500 responses). Can be overridden by subclasses who want\\n        customised 500 handling.\\n\\n        Be *very* careful when overriding this because the error could be\\n        caused by anything, so assuming something like the database is always\\n        available would be an error.\\n        \"\"\"\\n        from django.conf import settings\\n        from django.core.mail import mail_admins\\n\\n        if settings.DEBUG_PROPAGATE_EXCEPTIONS:\\n            raise\\n\\n        if settings.DEBUG:\\n            from django.views import debug\\n            return debug.technical_500_response(request, *exc_info)\\n\\n        # When DEBUG is False, send an error message to the admins.\\n        subject = \\'Error (%s IP): %s\\' % ((request.META.get(\\'REMOTE_ADDR\\') in settings.INTERNAL_IPS and \\'internal\\' or \\'EXTERNAL\\'), request.path)\\n        try:\\n            request_repr = repr(request)\\n        except:\\n            request_repr = \"Request repr() unavailable\"\\n        message = \"%s\\\\n\\\\n%s\" % (self._get_traceback(exc_info), request_repr)\\n        mail_admins(subject, message, fail_silently=True)\\n        # If Http500 handler is not installed, re-raise last exception\\n        if resolver.urlconf_module is None:\\n            raise exc_info[1], None, exc_info[2]\\n        # Return an HttpResponse that displays a friendly error message.\\n        callback, param_dict = resolver.resolve500()\\n        return callback(request, **param_dict)\\n\\n    def _get_traceback(self, exc_info=None):\\n        \"Helper function to return the traceback as a string\"\\n        import traceback\\n        return \\'\\\\n\\'.join(traceback.format_exception(*(exc_info or sys.exc_info())))\\n\\n    def apply_response_fixes(self, request, response):\\n        \"\"\"\\n        Applies each of the functions in self.response_fixes to the request and\\n        response, modifying the response in the process. Returns the new\\n        response.\\n        \"\"\"\\n        for func in self.response_fixes:\\n            response = func(request, response)\\n        return response\\n\\ndef get_script_name(environ):\\n    \"\"\"\\n    Returns the equivalent of the HTTP request\\'s SCRIPT_NAME environment\\n    variable. If Apache mod_rewrite has been used, returns what would have been\\n    the script name prior to any rewriting (so it\\'s the script name as seen\\n    from the client\\'s perspective), unless DJANGO_USE_POST_REWRITE is set (to\\n    anything).\\n    \"\"\"\\n    from django.conf import settings\\n    if settings.FORCE_SCRIPT_NAME is not None:\\n        return force_unicode(settings.FORCE_SCRIPT_NAME)\\n\\n    # If Apache\\'s mod_rewrite had a whack at the URL, Apache set either\\n    # SCRIPT_URL or REDIRECT_URL to the full resource URL before applying any\\n    # rewrites. Unfortunately not every Web server (lighttpd!) passes this\\n    # information through all the time, so FORCE_SCRIPT_NAME, above, is still\\n    # needed.\\n    script_url = environ.get(\\'SCRIPT_URL\\', u\\'\\')\\n    if not script_url:\\n        script_url = environ.get(\\'REDIRECT_URL\\', u\\'\\')\\n    if script_url:\\n        return force_unicode(script_url[:-len(environ.get(\\'PATH_INFO\\', \\'\\'))])\\n    return force_unicode(environ.get(\\'SCRIPT_NAME\\', u\\'\\'))\\n\\n', '\"\"\"\\nmbed SDK\\nCopyright (c) 2011-2013 ARM Limited\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\"\"\"\\n\\nimport os\\nimport sys\\nfrom os.path import join, basename, exists, abspath, dirname\\nfrom time import sleep\\nfrom host_test_plugins import HostTestPluginBase\\n\\nsys.path.append(abspath(join(dirname(__file__), \"../../../\")))\\nfrom tools.test_api import get_autodetected_MUTS_list\\n\\nclass HostTestPluginCopyMethod_Smart(HostTestPluginBase):\\n\\n    # Plugin interface\\n    name = \\'HostTestPluginCopyMethod_Smart\\'\\n    type = \\'CopyMethod\\'\\n    stable = True\\n    capabilities = [\\'smart\\']\\n    required_parameters = [\\'image_path\\', \\'destination_disk\\', \\'target_mcu\\']\\n\\n    def setup(self, *args, **kwargs):\\n        \"\"\" Configure plugin, this function should be called before plugin execute() method is used.\\n        \"\"\"\\n        return True\\n\\n    def execute(self, capability, *args, **kwargs):\\n        \"\"\" Executes capability by name.\\n            Each capability may directly just call some command line\\n            program or execute building pythonic function\\n        \"\"\"\\n        result = False\\n        if self.check_parameters(capability, *args, **kwargs) is True:\\n            image_path = kwargs[\\'image_path\\']\\n            destination_disk = kwargs[\\'destination_disk\\']\\n            target_mcu = kwargs[\\'target_mcu\\']\\n            # Wait for mount point to be ready\\n            self.check_mount_point_ready(destination_disk)  # Blocking\\n            # Prepare correct command line parameter values\\n            image_base_name = basename(image_path)\\n            destination_path = join(destination_disk, image_base_name)\\n            if capability == \\'smart\\':\\n                if os.name == \\'posix\\':\\n                    cmd = [\\'cp\\', image_path, destination_path]\\n                    result = self.run_command(cmd, shell=False)\\n\\n                    cmd = [\\'sync\\']\\n                    result = self.run_command(cmd, shell=False)\\n                elif os.name == \\'nt\\':\\n                    cmd = [\\'copy\\', image_path, destination_path]\\n                    result = self.run_command(cmd, shell=True)\\n\\n                # Give the OS and filesystem time to settle down\\n                sleep(3)\\n\\n                platform_name_filter = [target_mcu]\\n                muts_list = {}\\n\\n                remount_complete = False\\n\\n                for i in range(0, 60):\\n                    print(\\'Looking for %s with MBEDLS\\' % target_mcu)\\n                    muts_list = get_autodetected_MUTS_list(platform_name_filter=platform_name_filter)\\n\\n                    if 1 in muts_list:\\n                        mut = muts_list[1]\\n                        destination_disk = mut[\\'disk\\']\\n                        destination_path = join(destination_disk, image_base_name)\\n\\n                        if mut[\\'mcu\\'] == \\'LPC1768\\' or mut[\\'mcu\\'] == \\'LPC11U24\\':\\n                            if exists(destination_disk) and exists(destination_path):\\n                                remount_complete = True\\n                                break;\\n                        else:\\n                            if exists(destination_disk) and not exists(destination_path):\\n                                remount_complete = True\\n                                break;\\n\\n                    sleep(1)\\n\\n                if remount_complete:\\n                    print(\\'Remount complete\\')\\n                else:\\n                    print(\\'Remount FAILED\\')\\n\\n                    if exists(destination_disk):\\n                        print(\\'Disk exists\\')\\n                    else:\\n                        print(\\'Disk does not exist\\')\\n\\n                    if exists(destination_path):\\n                        print(\\'Image exists\\')\\n                    else:\\n                        print(\\'Image does not exist\\')\\n\\n                    result = None\\n\\n\\n        return result\\n\\ndef load_plugin():\\n    \"\"\" Returns plugin available in this module\\n    \"\"\"\\n    return HostTestPluginCopyMethod_Smart()\\n', '\"\"\"\\nThis file contains implementation override of SearchFilterGenerator which will allow\\n    * Filter by all courses in which the user is enrolled in\\n\"\"\"\\nfrom microsite_configuration import microsite\\n\\nfrom student.models import CourseEnrollment\\nfrom opaque_keys import InvalidKeyError\\nfrom opaque_keys.edx.keys import CourseKey\\nfrom opaque_keys.edx.locations import SlashSeparatedCourseKey\\nfrom xmodule.modulestore.django import modulestore\\n\\nfrom search.filter_generator import SearchFilterGenerator\\nfrom openedx.core.djangoapps.user_api.partition_schemes import RandomUserPartitionScheme\\nfrom openedx.core.djangoapps.course_groups.partition_scheme import CohortPartitionScheme\\nfrom courseware.access import get_user_role\\n\\n\\nINCLUDE_SCHEMES = [CohortPartitionScheme, RandomUserPartitionScheme, ]\\nSCHEME_SUPPORTS_ASSIGNMENT = [RandomUserPartitionScheme, ]\\n\\n\\nclass LmsSearchFilterGenerator(SearchFilterGenerator):\\n    \"\"\" SearchFilterGenerator for LMS Search \"\"\"\\n\\n    _user_enrollments = {}\\n\\n    def _enrollments_for_user(self, user):\\n        \"\"\" Return the specified user\\'s course enrollments \"\"\"\\n        if user not in self._user_enrollments:\\n            self._user_enrollments[user] = CourseEnrollment.enrollments_for_user(user)\\n        return self._user_enrollments[user]\\n\\n    def filter_dictionary(self, **kwargs):\\n        \"\"\" LMS implementation, adds filtering by user partition, course id and user \"\"\"\\n\\n        def get_group_for_user_partition(user_partition, course_key, user):\\n            \"\"\" Returns the specified user\\'s group for user partition \"\"\"\\n            if user_partition.scheme in SCHEME_SUPPORTS_ASSIGNMENT:\\n                return user_partition.scheme.get_group_for_user(\\n                    course_key,\\n                    user,\\n                    user_partition,\\n                    assign=False,\\n                )\\n            else:\\n                return user_partition.scheme.get_group_for_user(\\n                    course_key,\\n                    user,\\n                    user_partition,\\n                )\\n\\n        def get_group_ids_for_user(course, user):\\n            \"\"\" Collect user partition group ids for user for this course \"\"\"\\n            partition_groups = []\\n            for user_partition in course.user_partitions:\\n                if user_partition.scheme in INCLUDE_SCHEMES:\\n                    group = get_group_for_user_partition(user_partition, course.id, user)\\n                    if group:\\n                        partition_groups.append(group)\\n            partition_group_ids = [unicode(partition_group.id) for partition_group in partition_groups]\\n            return partition_group_ids if partition_group_ids else None\\n\\n        filter_dictionary = super(LmsSearchFilterGenerator, self).filter_dictionary(**kwargs)\\n        if \\'user\\' in kwargs:\\n            user = kwargs[\\'user\\']\\n\\n            if \\'course_id\\' in kwargs and kwargs[\\'course_id\\']:\\n                try:\\n                    course_key = CourseKey.from_string(kwargs[\\'course_id\\'])\\n                except InvalidKeyError:\\n                    course_key = SlashSeparatedCourseKey.from_deprecated_string(kwargs[\\'course_id\\'])\\n\\n                # Staff user looking at course as staff user\\n                if get_user_role(user, course_key) in (\\'instructor\\', \\'staff\\'):\\n                    return filter_dictionary\\n                # Need to check course exist (if course gets deleted enrollments don\\'t get cleaned up)\\n                course = modulestore().get_course(course_key)\\n                if course:\\n                    filter_dictionary[\\'content_groups\\'] = get_group_ids_for_user(course, user)\\n            else:\\n                user_enrollments = self._enrollments_for_user(user)\\n                content_groups = []\\n                for enrollment in user_enrollments:\\n                    course = modulestore().get_course(enrollment.course_id)\\n                    if course:\\n                        enrollment_group_ids = get_group_ids_for_user(course, user)\\n                        if enrollment_group_ids:\\n                            content_groups.extend(enrollment_group_ids)\\n\\n                filter_dictionary[\\'content_groups\\'] = content_groups if content_groups else None\\n\\n        return filter_dictionary\\n\\n    def field_dictionary(self, **kwargs):\\n        \"\"\" add course if provided otherwise add courses in which the user is enrolled in \"\"\"\\n        field_dictionary = super(LmsSearchFilterGenerator, self).field_dictionary(**kwargs)\\n        if not kwargs.get(\\'user\\'):\\n            field_dictionary[\\'course\\'] = []\\n        elif not kwargs.get(\\'course_id\\'):\\n            user_enrollments = self._enrollments_for_user(kwargs[\\'user\\'])\\n            field_dictionary[\\'course\\'] = [unicode(enrollment.course_id) for enrollment in user_enrollments]\\n\\n        # if we have an org filter, only include results for this org filter\\n        course_org_filter = microsite.get_value(\\'course_org_filter\\')\\n        if course_org_filter:\\n            field_dictionary[\\'org\\'] = course_org_filter\\n\\n        return field_dictionary\\n\\n    def exclude_dictionary(self, **kwargs):\\n        \"\"\" If we are not on a microsite, then exclude any microsites that are defined \"\"\"\\n        exclude_dictionary = super(LmsSearchFilterGenerator, self).exclude_dictionary(**kwargs)\\n        course_org_filter = microsite.get_value(\\'course_org_filter\\')\\n        # If we have a course filter we are ensuring that we only get those courses above\\n        if not course_org_filter:\\n            org_filter_out_set = microsite.get_all_orgs()\\n            if org_filter_out_set:\\n                exclude_dictionary[\\'org\\'] = list(org_filter_out_set)\\n\\n        return exclude_dictionary\\n', 'from __future__ import unicode_literals\\n\\nfrom .common import InfoExtractor\\nfrom ..utils import (\\n    parse_iso8601,\\n    int_or_none,\\n)\\n\\n\\nclass TwentyFourVideoIE(InfoExtractor):\\n    IE_NAME = \\'24video\\'\\n    _VALID_URL = r\\'https?://(?:www\\\\.)?24video\\\\.net/(?:video/(?:view|xml)/|player/new24_play\\\\.swf\\\\?id=)(?P<id>\\\\d+)\\'\\n\\n    _TESTS = [\\n        {\\n            \\'url\\': \\'http://www.24video.net/video/view/1044982\\',\\n            \\'md5\\': \\'48dd7646775690a80447a8dca6a2df76\\',\\n            \\'info_dict\\': {\\n                \\'id\\': \\'1044982\\',\\n                \\'ext\\': \\'mp4\\',\\n                \\'title\\': \\'Эротика каменного века\\',\\n                \\'description\\': \\'Как смотрели порно в каменном веке.\\',\\n                \\'thumbnail\\': \\'re:^https?://.*\\\\.jpg$\\',\\n                \\'uploader\\': \\'SUPERTELO\\',\\n                \\'duration\\': 31,\\n                \\'timestamp\\': 1275937857,\\n                \\'upload_date\\': \\'20100607\\',\\n                \\'age_limit\\': 18,\\n                \\'like_count\\': int,\\n                \\'dislike_count\\': int,\\n            },\\n        },\\n        {\\n            \\'url\\': \\'http://www.24video.net/player/new24_play.swf?id=1044982\\',\\n            \\'only_matching\\': True,\\n        }\\n    ]\\n\\n    def _real_extract(self, url):\\n        video_id = self._match_id(url)\\n\\n        webpage = self._download_webpage(\\n            \\'http://www.24video.net/video/view/%s\\' % video_id, video_id)\\n\\n        title = self._og_search_title(webpage)\\n        description = self._html_search_regex(\\n            r\\'<span itemprop=\"description\">([^<]+)</span>\\', webpage, \\'description\\', fatal=False)\\n        thumbnail = self._og_search_thumbnail(webpage)\\n        duration = int_or_none(self._og_search_property(\\n            \\'duration\\', webpage, \\'duration\\', fatal=False))\\n        timestamp = parse_iso8601(self._search_regex(\\n            r\\'<time id=\"video-timeago\" datetime=\"([^\"]+)\" itemprop=\"uploadDate\">\\',\\n            webpage, \\'upload date\\'))\\n\\n        uploader = self._html_search_regex(\\n            r\\'Загрузил\\\\s*<a href=\"/jsecUser/movies/[^\"]+\" class=\"link\">([^<]+)</a>\\',\\n            webpage, \\'uploader\\', fatal=False)\\n\\n        view_count = int_or_none(self._html_search_regex(\\n            r\\'<span class=\"video-views\">(\\\\d+) просмотр\\',\\n            webpage, \\'view count\\', fatal=False))\\n        comment_count = int_or_none(self._html_search_regex(\\n            r\\'<div class=\"comments-title\" id=\"comments-count\">(\\\\d+) комментари\\',\\n            webpage, \\'comment count\\', fatal=False))\\n\\n        formats = []\\n\\n        pc_video = self._download_xml(\\n            \\'http://www.24video.net/video/xml/%s?mode=play\\' % video_id,\\n            video_id, \\'Downloading PC video URL\\').find(\\'.//video\\')\\n\\n        formats.append({\\n            \\'url\\': pc_video.attrib[\\'url\\'],\\n            \\'format_id\\': \\'pc\\',\\n            \\'quality\\': 1,\\n        })\\n\\n        like_count = int_or_none(pc_video.get(\\'ratingPlus\\'))\\n        dislike_count = int_or_none(pc_video.get(\\'ratingMinus\\'))\\n        age_limit = 18 if pc_video.get(\\'adult\\') == \\'true\\' else 0\\n\\n        mobile_video = self._download_xml(\\n            \\'http://www.24video.net/video/xml/%s\\' % video_id,\\n            video_id, \\'Downloading mobile video URL\\').find(\\'.//video\\')\\n\\n        formats.append({\\n            \\'url\\': mobile_video.attrib[\\'url\\'],\\n            \\'format_id\\': \\'mobile\\',\\n            \\'quality\\': 0,\\n        })\\n\\n        self._sort_formats(formats)\\n\\n        return {\\n            \\'id\\': video_id,\\n            \\'title\\': title,\\n            \\'description\\': description,\\n            \\'thumbnail\\': thumbnail,\\n            \\'uploader\\': uploader,\\n            \\'duration\\': duration,\\n            \\'timestamp\\': timestamp,\\n            \\'view_count\\': view_count,\\n            \\'comment_count\\': comment_count,\\n            \\'like_count\\': like_count,\\n            \\'dislike_count\\': dislike_count,\\n            \\'age_limit\\': age_limit,\\n            \\'formats\\': formats,\\n        }\\n', \"from __future__ import unicode_literals\\n\\nimport re\\n\\nfrom .common import InfoExtractor\\nfrom ..compat import compat_str\\nfrom ..utils import (\\n    int_or_none,\\n    js_to_json,\\n    strip_or_none,\\n    try_get,\\n    unified_timestamp,\\n)\\n\\n\\nclass WatchBoxIE(InfoExtractor):\\n    _VALID_URL = r'https?://(?:www\\\\.)?watchbox\\\\.de/(?P<kind>serien|filme)/(?:[^/]+/)*[^/]+-(?P<id>\\\\d+)'\\n    _TESTS = [{\\n        # film\\n        'url': 'https://www.watchbox.de/filme/free-jimmy-12325.html',\\n        'info_dict': {\\n            'id': '341368',\\n            'ext': 'mp4',\\n            'title': 'Free Jimmy',\\n            'description': 'md5:bcd8bafbbf9dc0ef98063d344d7cc5f6',\\n            'thumbnail': r're:^https?://.*\\\\.jpg$',\\n            'duration': 4890,\\n            'age_limit': 16,\\n            'release_year': 2009,\\n        },\\n        'params': {\\n            'format': 'bestvideo',\\n            'skip_download': True,\\n        },\\n        'expected_warnings': ['Failed to download m3u8 information'],\\n    }, {\\n        # episode\\n        'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-1/date-in-der-hoelle-328286.html',\\n        'info_dict': {\\n            'id': '328286',\\n            'ext': 'mp4',\\n            'title': 'S01 E01 - Date in der Hölle',\\n            'description': 'md5:2f31c74a8186899f33cb5114491dae2b',\\n            'thumbnail': r're:^https?://.*\\\\.jpg$',\\n            'duration': 1291,\\n            'age_limit': 12,\\n            'release_year': 2010,\\n            'series': 'Ugly Americans',\\n            'season_number': 1,\\n            'episode': 'Date in der Hölle',\\n            'episode_number': 1,\\n        },\\n        'params': {\\n            'format': 'bestvideo',\\n            'skip_download': True,\\n        },\\n        'expected_warnings': ['Failed to download m3u8 information'],\\n    }, {\\n        'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-2/der-ring-des-powers-328270',\\n        'only_matching': True,\\n    }]\\n\\n    def _real_extract(self, url):\\n        mobj = re.match(self._VALID_URL, url)\\n        kind, video_id = mobj.group('kind', 'id')\\n\\n        webpage = self._download_webpage(url, video_id)\\n\\n        source = self._parse_json(\\n            self._search_regex(\\n                r'(?s)source\\\\s*:\\\\s*({.+?})\\\\s*,\\\\s*\\\\n', webpage, 'source',\\n                default='{}'),\\n            video_id, transform_source=js_to_json, fatal=False) or {}\\n\\n        video_id = compat_str(source.get('videoId') or video_id)\\n\\n        devapi = self._download_json(\\n            'http://api.watchbox.de/devapi/id/%s' % video_id, video_id, query={\\n                'format': 'json',\\n                'apikey': 'hbbtv',\\n            }, fatal=False)\\n\\n        item = try_get(devapi, lambda x: x['items'][0], dict) or {}\\n\\n        title = item.get('title') or try_get(\\n            item, lambda x: x['movie']['headline_movie'],\\n            compat_str) or source['title']\\n\\n        formats = []\\n        hls_url = item.get('media_videourl_hls') or source.get('hls')\\n        if hls_url:\\n            formats.extend(self._extract_m3u8_formats(\\n                hls_url, video_id, 'mp4', entry_protocol='m3u8_native',\\n                m3u8_id='hls', fatal=False))\\n        dash_url = item.get('media_videourl_wv') or source.get('dash')\\n        if dash_url:\\n            formats.extend(self._extract_mpd_formats(\\n                dash_url, video_id, mpd_id='dash', fatal=False))\\n        mp4_url = item.get('media_videourl')\\n        if mp4_url:\\n            formats.append({\\n                'url': mp4_url,\\n                'format_id': 'mp4',\\n                'width': int_or_none(item.get('width')),\\n                'height': int_or_none(item.get('height')),\\n                'tbr': int_or_none(item.get('bitrate')),\\n            })\\n        self._sort_formats(formats)\\n\\n        description = strip_or_none(item.get('descr'))\\n        thumbnail = item.get('media_content_thumbnail_large') or source.get('poster') or item.get('media_thumbnail')\\n        duration = int_or_none(item.get('media_length') or source.get('length'))\\n        timestamp = unified_timestamp(item.get('pubDate'))\\n        view_count = int_or_none(item.get('media_views'))\\n        age_limit = int_or_none(try_get(item, lambda x: x['movie']['fsk']))\\n        release_year = int_or_none(try_get(item, lambda x: x['movie']['rel_year']))\\n\\n        info = {\\n            'id': video_id,\\n            'title': title,\\n            'description': description,\\n            'thumbnail': thumbnail,\\n            'duration': duration,\\n            'timestamp': timestamp,\\n            'view_count': view_count,\\n            'age_limit': age_limit,\\n            'release_year': release_year,\\n            'formats': formats,\\n        }\\n\\n        if kind.lower() == 'serien':\\n            series = try_get(\\n                item, lambda x: x['special']['title'],\\n                compat_str) or source.get('format')\\n            season_number = int_or_none(self._search_regex(\\n                r'^S(\\\\d{1,2})\\\\s*E\\\\d{1,2}', title, 'season number',\\n                default=None) or self._search_regex(\\n                    r'/staffel-(\\\\d+)/', url, 'season number', default=None))\\n            episode = source.get('title')\\n            episode_number = int_or_none(self._search_regex(\\n                r'^S\\\\d{1,2}\\\\s*E(\\\\d{1,2})', title, 'episode number',\\n                default=None))\\n            info.update({\\n                'series': series,\\n                'season_number': season_number,\\n                'episode': episode,\\n                'episode_number': episode_number,\\n            })\\n\\n        return info\\n\", '\"\"\"\\nModule that takes control of versioning.\\n\\n@author: Stijn De Weirdt (Ghent University)\\n@author: Dries Verdegem (Ghent University)\\n@author: Kenneth Hoste (Ghent University)\\n@author: Pieter De Baets (Ghent University)\\n@author: Jens Timmerman (Ghent University)\\n\"\"\"\\nimport os\\nfrom distutils.version import LooseVersion\\nfrom socket import gethostname\\n\\nVERSION = LooseVersion(\"1.14.0\")\\nUNKNOWN = \"UNKNOWN\"\\n\\ndef get_git_revision():\\n    \"\"\"\\n    Returns the git revision (e.g. aab4afc016b742c6d4b157427e192942d0e131fe),\\n    or UNKNOWN is getting the git revision fails\\n\\n    relies on GitPython (see http://gitorious.org/git-python)\\n    \"\"\"\\n    try:\\n        import git\\n    except ImportError:\\n        return UNKNOWN\\n    try:\\n        path = os.path.dirname(__file__)\\n        gitrepo = git.Git(path)\\n        return gitrepo.rev_list(\"HEAD\").splitlines()[0]\\n    except git.GitCommandError:\\n        return UNKNOWN\\n\\ngit_rev = get_git_revision()\\nif git_rev == UNKNOWN:\\n    VERBOSE_VERSION = VERSION\\nelse:\\n    VERBOSE_VERSION = LooseVersion(\"%s-r%s\" % (VERSION, get_git_revision()))\\n\\nFRAMEWORK_VERSION = VERBOSE_VERSION\\n\\ntry:\\n    from easybuild.easyblocks import VERBOSE_VERSION as EASYBLOCKS_VERSION\\nexcept:\\n    EASYBLOCKS_VERSION = \\'0.0.UNKNOWN.EASYBLOCKS\\'  # make sure it is smaller then anything\\n\\ndef this_is_easybuild():\\n    \"\"\"Standard starting message\"\"\"\\n    top_version = max(FRAMEWORK_VERSION, EASYBLOCKS_VERSION)\\n    # !!! bootstrap_eb.py script checks hard on the string below, so adjust with sufficient care !!!\\n    msg = \"This is EasyBuild %s (framework: %s, easyblocks: %s) on host %s.\" \\\\\\n         % (top_version, FRAMEWORK_VERSION, EASYBLOCKS_VERSION, gethostname())\\n\\n    return msg\\n', 'import os\\nimport sys\\nimport getopt\\nimport shutil\\nimport string\\n\\nquiet = 0\\ntest = 0\\ncomments = 0\\n\\nsysfsprefix = \"/sys/devices/system/rttest/rttest\"\\nstatusfile = \"/status\"\\ncommandfile = \"/command\"\\n\\ncmd_opcodes = {\\n    \"schedother\"    : \"1\",\\n    \"schedfifo\"     : \"2\",\\n    \"lock\"          : \"3\",\\n    \"locknowait\"    : \"4\",\\n    \"lockint\"       : \"5\",\\n    \"lockintnowait\" : \"6\",\\n    \"lockcont\"      : \"7\",\\n    \"unlock\"        : \"8\",\\n    \"lockbkl\"       : \"9\",\\n    \"unlockbkl\"     : \"10\",\\n    \"signal\"        : \"11\",\\n    \"resetevent\"    : \"98\",\\n    \"reset\"         : \"99\",\\n    }\\n\\ntest_opcodes = {\\n    \"prioeq\"        : [\"P\" , \"eq\" , None],\\n    \"priolt\"        : [\"P\" , \"lt\" , None],\\n    \"priogt\"        : [\"P\" , \"gt\" , None],\\n    \"nprioeq\"       : [\"N\" , \"eq\" , None],\\n    \"npriolt\"       : [\"N\" , \"lt\" , None],\\n    \"npriogt\"       : [\"N\" , \"gt\" , None],\\n    \"unlocked\"      : [\"M\" , \"eq\" , 0],\\n    \"trylock\"       : [\"M\" , \"eq\" , 1],\\n    \"blocked\"       : [\"M\" , \"eq\" , 2],\\n    \"blockedwake\"   : [\"M\" , \"eq\" , 3],\\n    \"locked\"        : [\"M\" , \"eq\" , 4],\\n    \"opcodeeq\"      : [\"O\" , \"eq\" , None],\\n    \"opcodelt\"      : [\"O\" , \"lt\" , None],\\n    \"opcodegt\"      : [\"O\" , \"gt\" , None],\\n    \"eventeq\"       : [\"E\" , \"eq\" , None],\\n    \"eventlt\"       : [\"E\" , \"lt\" , None],\\n    \"eventgt\"       : [\"E\" , \"gt\" , None],\\n    }\\n\\ndef usage():\\n    print \"rt-tester.py <-c -h -q -t> <testfile>\"\\n    print \" -c    display comments after first command\"\\n    print \" -h    help\"\\n    print \" -q    quiet mode\"\\n    print \" -t    test mode (syntax check)\"\\n    print \" testfile: read test specification from testfile\"\\n    print \" otherwise from stdin\"\\n    return\\n\\ndef progress(str):\\n    if not quiet:\\n        print str\\n\\ndef analyse(val, top, arg):\\n\\n    intval = int(val)\\n\\n    if top[0] == \"M\":\\n        intval = intval / (10 ** int(arg))\\n\\tintval = intval % 10\\n        argval = top[2]\\n    elif top[0] == \"O\":\\n        argval = int(cmd_opcodes.get(arg, arg))\\n    else:\\n        argval = int(arg)\\n\\n    # progress(\"%d %s %d\" %(intval, top[1], argval))\\n\\n    if top[1] == \"eq\" and intval == argval:\\n\\treturn 1\\n    if top[1] == \"lt\" and intval < argval:\\n        return 1\\n    if top[1] == \"gt\" and intval > argval:\\n\\treturn 1\\n    return 0\\n\\ntry:\\n    (options, arguments) = getopt.getopt(sys.argv[1:],\\'chqt\\')\\nexcept getopt.GetoptError, ex:\\n    usage()\\n    sys.exit(1)\\n\\nfor option, value in options:\\n    if option == \"-c\":\\n        comments = 1\\n    elif option == \"-q\":\\n        quiet = 1\\n    elif option == \"-t\":\\n        test = 1\\n    elif option == \\'-h\\':\\n        usage()\\n        sys.exit(0)\\n\\nif arguments:\\n    try:\\n        fd = open(arguments[0])\\n    except Exception,ex:\\n        sys.stderr.write(\"File not found %s\\\\n\" %(arguments[0]))\\n        sys.exit(1)\\nelse:\\n    fd = sys.stdin\\n\\nlinenr = 0\\n\\nwhile 1:\\n\\n    linenr = linenr + 1\\n    line = fd.readline()\\n    if not len(line):\\n        break\\n\\n    line = line.strip()\\n    parts = line.split(\":\")\\n\\n    if not parts or len(parts) < 1:\\n        continue\\n\\n    if len(parts[0]) == 0:\\n        continue\\n\\n    if parts[0].startswith(\"#\"):\\n\\tif comments > 1:\\n\\t    progress(line)\\n\\tcontinue\\n\\n    if comments == 1:\\n\\tcomments = 2\\n\\n    progress(line)\\n\\n    cmd = parts[0].strip().lower()\\n    opc = parts[1].strip().lower()\\n    tid = parts[2].strip()\\n    dat = parts[3].strip()\\n\\n    try:\\n        # Test or wait for a status value\\n        if cmd == \"t\" or cmd == \"w\":\\n            testop = test_opcodes[opc]\\n\\n            fname = \"%s%s%s\" %(sysfsprefix, tid, statusfile)\\n            if test:\\n\\t\\tprint fname\\n                continue\\n\\n            while 1:\\n                query = 1\\n                fsta = open(fname, \\'r\\')\\n                status = fsta.readline().strip()\\n                fsta.close()\\n                stat = status.split(\",\")\\n                for s in stat:\\n\\t\\t    s = s.strip()\\n                    if s.startswith(testop[0]):\\n                        # Seperate status value\\n                        val = s[2:].strip()\\n                        query = analyse(val, testop, dat)\\n                        break\\n                if query or cmd == \"t\":\\n                    break\\n\\n            progress(\"   \" + status)\\n\\n            if not query:\\n                sys.stderr.write(\"Test failed in line %d\\\\n\" %(linenr))\\n\\t\\tsys.exit(1)\\n\\n        # Issue a command to the tester\\n        elif cmd == \"c\":\\n            cmdnr = cmd_opcodes[opc]\\n            # Build command string and sys filename\\n            cmdstr = \"%s:%s\" %(cmdnr, dat)\\n            fname = \"%s%s%s\" %(sysfsprefix, tid, commandfile)\\n            if test:\\n\\t\\tprint fname\\n                continue\\n            fcmd = open(fname, \\'w\\')\\n            fcmd.write(cmdstr)\\n            fcmd.close()\\n\\n    except Exception,ex:\\n    \\tsys.stderr.write(str(ex))\\n        sys.stderr.write(\"\\\\nSyntax error in line %d\\\\n\" %(linenr))\\n        if not test:\\n            fd.close()\\n            sys.exit(1)\\n\\nprint \"Pass\"\\nsys.exit(0)\\n\\n\\n', '\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nimport os\\nimport json\\nimport pytest\\nimport sys\\n\\nif sys.version_info < (2, 7):\\n    pytestmark = pytest.mark.skip(\"F5 Ansible modules require Python >= 2.7\")\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\n\\ntry:\\n    from library.modules.bigip_service_policy import ApiParameters\\n    from library.modules.bigip_service_policy import ModuleParameters\\n    from library.modules.bigip_service_policy import ModuleManager\\n    from library.modules.bigip_service_policy import ArgumentSpec\\n\\n    # In Ansible 2.8, Ansible changed import paths.\\n    from test.units.compat import unittest\\n    from test.units.compat.mock import Mock\\n    from test.units.compat.mock import patch\\n\\n    from test.units.modules.utils import set_module_args\\nexcept ImportError:\\n    from ansible.modules.network.f5.bigip_service_policy import ApiParameters\\n    from ansible.modules.network.f5.bigip_service_policy import ModuleParameters\\n    from ansible.modules.network.f5.bigip_service_policy import ModuleManager\\n    from ansible.modules.network.f5.bigip_service_policy import ArgumentSpec\\n\\n    # Ansible 2.8 imports\\n    from units.compat import unittest\\n    from units.compat.mock import Mock\\n    from units.compat.mock import patch\\n\\n    from units.modules.utils import set_module_args\\n\\n\\nfixture_path = os.path.join(os.path.dirname(__file__), \\'fixtures\\')\\nfixture_data = {}\\n\\n\\ndef load_fixture(name):\\n    path = os.path.join(fixture_path, name)\\n\\n    if path in fixture_data:\\n        return fixture_data[path]\\n\\n    with open(path) as f:\\n        data = f.read()\\n\\n    try:\\n        data = json.loads(data)\\n    except Exception:\\n        pass\\n\\n    fixture_data[path] = data\\n    return data\\n\\n\\nclass TestParameters(unittest.TestCase):\\n    def test_module_parameters(self):\\n        args = dict(\\n            name=\\'foo\\',\\n            description=\\'my description\\',\\n            timer_policy=\\'timer1\\',\\n            port_misuse_policy=\\'misuse1\\',\\n        )\\n\\n        p = ModuleParameters(params=args)\\n        assert p.name == \\'foo\\'\\n        assert p.description == \\'my description\\'\\n        assert p.timer_policy == \\'/Common/timer1\\'\\n        assert p.port_misuse_policy == \\'/Common/misuse1\\'\\n\\n    def test_api_parameters(self):\\n        args = load_fixture(\\'load_net_service_policy_1.json\\')\\n        p = ApiParameters(params=args)\\n        assert p.name == \\'baz\\'\\n        assert p.description == \\'my description\\'\\n        assert p.timer_policy == \\'/Common/foo\\'\\n        assert p.port_misuse_policy == \\'/Common/bar\\'\\n\\n\\nclass TestManager(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.spec = ArgumentSpec()\\n        try:\\n            self.p1 = patch(\\'library.modules.bigip_service_policy.module_provisioned\\')\\n            self.m1 = self.p1.start()\\n            self.m1.return_value = True\\n        except Exception:\\n            self.p1 = patch(\\'ansible.modules.network.f5.bigip_service_policy.module_provisioned\\')\\n            self.m1 = self.p1.start()\\n            self.m1.return_value = True\\n\\n    def test_create_selfip(self, *args):\\n        set_module_args(dict(\\n            name=\\'foo\\',\\n            description=\\'my description\\',\\n            timer_policy=\\'timer1\\',\\n            port_misuse_policy=\\'misuse1\\',\\n            partition=\\'Common\\',\\n            state=\\'present\\',\\n            provider=dict(\\n                server=\\'localhost\\',\\n                password=\\'password\\',\\n                user=\\'admin\\'\\n            )\\n        ))\\n\\n        module = AnsibleModule(\\n            argument_spec=self.spec.argument_spec,\\n            supports_check_mode=self.spec.supports_check_mode\\n        )\\n        mm = ModuleManager(module=module)\\n\\n        # Override methods to force specific logic in the module to happen\\n        mm.exists = Mock(side_effect=[False, True])\\n        mm.create_on_device = Mock(return_value=True)\\n        mm.module_provisioned = Mock(return_value=True)\\n\\n        results = mm.exec_module()\\n\\n        assert results[\\'changed\\'] is True\\n', '\\nimport matplotlib.pyplot as plt\\n\\nfrom zipline.algorithm import TradingAlgorithm\\nfrom zipline.utils.factory import load_from_yahoo\\n\\nfrom zipline.transforms.ta import EMA\\n\\nfrom datetime import datetime\\nimport pytz\\n\\n\\nclass DualEMATaLib(TradingAlgorithm):\\n    \"\"\"Dual Moving Average Crossover algorithm.\\n\\n    This algorithm buys apple once its short moving average crosses\\n    its long moving average (indicating upwards momentum) and sells\\n    its shares once the averages cross again (indicating downwards\\n    momentum).\\n\\n    \"\"\"\\n    def initialize(self, short_window=20, long_window=40):\\n        # Add 2 mavg transforms, one with a long window, one\\n        # with a short window.\\n        self.short_ema_trans = EMA(timeperiod=short_window)\\n        self.long_ema_trans = EMA(timeperiod=long_window)\\n\\n        # To keep track of whether we invested in the stock or not\\n        self.invested = False\\n\\n    def handle_data(self, data):\\n        self.short_ema = self.short_ema_trans.handle_data(data)\\n        self.long_ema = self.long_ema_trans.handle_data(data)\\n        if self.short_ema is None or self.long_ema is None:\\n            return\\n\\n        self.buy = False\\n        self.sell = False\\n\\n        if (self.short_ema > self.long_ema).all() and not self.invested:\\n            self.order(\\'AAPL\\', 100)\\n            self.invested = True\\n            self.buy = True\\n        elif (self.short_ema < self.long_ema).all() and self.invested:\\n            self.order(\\'AAPL\\', -100)\\n            self.invested = False\\n            self.sell = True\\n\\n        self.record(AAPL=data[\\'AAPL\\'].price,\\n                    short_ema=self.short_ema[\\'AAPL\\'],\\n                    long_ema=self.long_ema[\\'AAPL\\'],\\n                    buy=self.buy,\\n                    sell=self.sell)\\n\\nif __name__ == \\'__main__\\':\\n    start = datetime(1990, 1, 1, 0, 0, 0, 0, pytz.utc)\\n    end = datetime(1991, 1, 1, 0, 0, 0, 0, pytz.utc)\\n    data = load_from_yahoo(stocks=[\\'AAPL\\'], indexes={}, start=start,\\n                           end=end)\\n\\n    dma = DualEMATaLib()\\n    results = dma.run(data).dropna()\\n\\n    fig = plt.figure()\\n    ax1 = fig.add_subplot(211, ylabel=\\'portfolio value\\')\\n    results.portfolio_value.plot(ax=ax1)\\n\\n    ax2 = fig.add_subplot(212)\\n    results[[\\'AAPL\\', \\'short_ema\\', \\'long_ema\\']].plot(ax=ax2)\\n\\n    ax2.plot(results.ix[results.buy].index, results.short_ema[results.buy],\\n             \\'^\\', markersize=10, color=\\'m\\')\\n    ax2.plot(results.ix[results.sell].index, results.short_ema[results.sell],\\n             \\'v\\', markersize=10, color=\\'k\\')\\n    plt.legend(loc=0)\\n    plt.gcf().set_size_inches(18, 8)\\n'], 'license': ['apache-2.0', 'mit', 'apache-2.0', 'mpl-2.0', 'bsd-3-clause', 'bsd-3-clause', 'apache-2.0', 'gpl-3.0', 'apache-2.0', 'agpl-3.0', 'gpl-3.0', 'unlicense', 'gpl-2.0', 'gpl-2.0', 'gpl-3.0', 'apache-2.0'], 'hash': tensor([ 4058546181967095110, -8321461546816174659,  5893400732939769196,\n",
      "         -920462829349169424, -8993424027276596643,  2272791862150649423,\n",
      "         9129429547262664033,  7864434311480931289,  6523961272627054749,\n",
      "         7445365950426380491,  5228835369578569741,  2893657285305545592,\n",
      "        -6282021567314024766, -8372066044332046382, -4717631936433851971,\n",
      "        -1037034474115704284]), 'line_mean': tensor([41.3719, 37.9353, 37.7336, 35.3333, 35.4429, 33.9280, 31.9006, 44.1182,\n",
      "        36.1017, 45.5620, 34.0183, 35.6689, 34.2530, 23.1712, 30.2727, 34.1087],\n",
      "       dtype=torch.float64), 'line_max': tensor([ 80,  79, 137,  98,  80, 109,  74, 143, 101, 107, 114, 116, 100,  70,\n",
      "         97,  77]), 'alpha_frac': tensor([0.6290, 0.6451, 0.5446, 0.6272, 0.5735, 0.5296, 0.5372, 0.5984, 0.5539,\n",
      "        0.6324, 0.5174, 0.5068, 0.7198, 0.4933, 0.6449, 0.6449],\n",
      "       dtype=torch.float64), 'autogenerated': tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 13\u001b[0m, in \u001b[0;36mval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mloss)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_ETAG_TIMEOUT\"]     = \"500\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider setting up model checkpointing (set up a directory to save checkpoints)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear residual gradients (might cause issues with taking grad. of frozen layers)\n",
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    # TODO: Implement training and validation\n",
    "    ...\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common antidotes to CUDA Out of Memory errors include:\n",
    "1. Freezing layers of your model (training less parameters).\n",
    "2. Using gradient checkpointing to save GPU memory.\n",
    "3. Reducing the max sequence length of your data (default=1024 with GPT-2 tokenizer, which is colossal).\n",
    "4. Reducing batch size (look into gradient accumulation).\n",
    "\n",
    "And, of course:\n",
    "\n",
    "5. Using a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the model\n",
    "...\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
